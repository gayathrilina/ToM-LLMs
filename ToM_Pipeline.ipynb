{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "720198d63246454a81b1895c27c881aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f2dc854802c4d46b2a6365128265613",
              "IPY_MODEL_293009fa2f8542e2a968a83c0d04faa5",
              "IPY_MODEL_2db53195b41f429e8043982acd1918e7"
            ],
            "layout": "IPY_MODEL_9485dbaf8cc54b1f9d107791d90c047b"
          }
        },
        "8f2dc854802c4d46b2a6365128265613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20695d1143dc465eb5b72cba0d320a7d",
            "placeholder": "​",
            "style": "IPY_MODEL_ee32cf3775cf4c178ca7ec46a5c6691a",
            "value": "Downloading (…)olve/main/source.spm: 100%"
          }
        },
        "293009fa2f8542e2a968a83c0d04faa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_009f1146c7fd49f6b6bb6289051dd2cc",
            "max": 829382,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e49f8cf890f4f3d9304c457f4b51148",
            "value": 829382
          }
        },
        "2db53195b41f429e8043982acd1918e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3390ba2e43ba406dbfda071eabde4071",
            "placeholder": "​",
            "style": "IPY_MODEL_7e17f19ee32749c7b34d6949df8f3487",
            "value": " 829k/829k [00:00&lt;00:00, 11.1MB/s]"
          }
        },
        "9485dbaf8cc54b1f9d107791d90c047b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20695d1143dc465eb5b72cba0d320a7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee32cf3775cf4c178ca7ec46a5c6691a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "009f1146c7fd49f6b6bb6289051dd2cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e49f8cf890f4f3d9304c457f4b51148": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3390ba2e43ba406dbfda071eabde4071": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e17f19ee32749c7b34d6949df8f3487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f1c6feb61e24022b6faa752118aed7d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a1d101e7d93c45b8a669eca5f8590d9e",
              "IPY_MODEL_39bc92d527514ecbba045783a2dd8248",
              "IPY_MODEL_d9b974c7dd4d45888f89866cce809cc3"
            ],
            "layout": "IPY_MODEL_941806cad5e34cb6bf26bf5913e5efd9"
          }
        },
        "a1d101e7d93c45b8a669eca5f8590d9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9fd08978e954f89b36e26d21be9031b",
            "placeholder": "​",
            "style": "IPY_MODEL_962135d5d4d74d6d804fa57afb573b01",
            "value": "Downloading (…)olve/main/target.spm: 100%"
          }
        },
        "39bc92d527514ecbba045783a2dd8248": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff7c43b2eedc4c20ba6dc64ef2a54697",
            "max": 1060731,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_39f6a6f68040406096eaee6e243fbaec",
            "value": 1060731
          }
        },
        "d9b974c7dd4d45888f89866cce809cc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ac90f244135b437dacba2a5460777eae",
            "placeholder": "​",
            "style": "IPY_MODEL_2ba85096f3b9456f9944e43423c97113",
            "value": " 1.06M/1.06M [00:00&lt;00:00, 4.01MB/s]"
          }
        },
        "941806cad5e34cb6bf26bf5913e5efd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9fd08978e954f89b36e26d21be9031b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "962135d5d4d74d6d804fa57afb573b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ff7c43b2eedc4c20ba6dc64ef2a54697": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39f6a6f68040406096eaee6e243fbaec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ac90f244135b437dacba2a5460777eae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2ba85096f3b9456f9944e43423c97113": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2898c57a334c47399564cc5235be16e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_83efafc2959045fe8457ea1cdf064fb7",
              "IPY_MODEL_eaac52be3cb840058c71e411bf5a09b4",
              "IPY_MODEL_723f9e61aa9a43f79ccfc6bce1526af7"
            ],
            "layout": "IPY_MODEL_a882598b92d74db69482b8a81324ae28"
          }
        },
        "83efafc2959045fe8457ea1cdf064fb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c3c966f909e49b881c70ce3dc7f1c0f",
            "placeholder": "​",
            "style": "IPY_MODEL_70d994d308264e0a95961ac5e75ea986",
            "value": "Downloading (…)olve/main/vocab.json: 100%"
          }
        },
        "eaac52be3cb840058c71e411bf5a09b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6da44a9a6c2f48bcada0a4bee1ffdbb0",
            "max": 2619455,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c88e9f23e37e4f05878bdd34ed2392ab",
            "value": 2619455
          }
        },
        "723f9e61aa9a43f79ccfc6bce1526af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3613e92c8c844fa2913942f6a6eefadf",
            "placeholder": "​",
            "style": "IPY_MODEL_7fe3588ea75f42168bfc2ed78d0c2317",
            "value": " 2.62M/2.62M [00:00&lt;00:00, 7.84MB/s]"
          }
        },
        "a882598b92d74db69482b8a81324ae28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c3c966f909e49b881c70ce3dc7f1c0f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70d994d308264e0a95961ac5e75ea986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6da44a9a6c2f48bcada0a4bee1ffdbb0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c88e9f23e37e4f05878bdd34ed2392ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3613e92c8c844fa2913942f6a6eefadf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fe3588ea75f42168bfc2ed78d0c2317": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3b307d1a160a450aad0eae84e86e3111": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c52128a6f979408facb62103fc83898f",
              "IPY_MODEL_6bdffe4ab9ca413092c13a4ab2985a09",
              "IPY_MODEL_b2c638bd0c4149c7990c14f0952d07a0"
            ],
            "layout": "IPY_MODEL_1194c50d48044cedb3c97f8cbf08e4f0"
          }
        },
        "c52128a6f979408facb62103fc83898f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_103ec6ce00d341fa8c42955e5e84bb20",
            "placeholder": "​",
            "style": "IPY_MODEL_8e597b491cd649d19871736afd0e5c2b",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "6bdffe4ab9ca413092c13a4ab2985a09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2858f608f1fb4863bb2868831e9b8695",
            "max": 42,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04941a762b8d46cebedbf4190c8b7fbc",
            "value": 42
          }
        },
        "b2c638bd0c4149c7990c14f0952d07a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6250347f541248299c1e0a82c5ece0cf",
            "placeholder": "​",
            "style": "IPY_MODEL_f417cbc082814517bde9e433e0a86983",
            "value": " 42.0/42.0 [00:00&lt;00:00, 1.58kB/s]"
          }
        },
        "1194c50d48044cedb3c97f8cbf08e4f0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "103ec6ce00d341fa8c42955e5e84bb20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e597b491cd649d19871736afd0e5c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2858f608f1fb4863bb2868831e9b8695": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04941a762b8d46cebedbf4190c8b7fbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6250347f541248299c1e0a82c5ece0cf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f417cbc082814517bde9e433e0a86983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ade8e18590245b6afc4e102b59adfd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df3663fef606404ebc1988c68fba2ef6",
              "IPY_MODEL_c3e1aa6d7c7447e897f20befd60b1df6",
              "IPY_MODEL_5dd56e80c76e4703b9a4f8aa376035af"
            ],
            "layout": "IPY_MODEL_47fd626db1b643b08d5d0ed67de2c2c8"
          }
        },
        "df3663fef606404ebc1988c68fba2ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_610de9413f2f41c299e140b5736afc69",
            "placeholder": "​",
            "style": "IPY_MODEL_ad8ffacca2984688aa2ec376fcded94a",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "c3e1aa6d7c7447e897f20befd60b1df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_354d353579764f198407324b71411615",
            "max": 1381,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_394092ce70cc47f3a2a6a6cf894a95dd",
            "value": 1381
          }
        },
        "5dd56e80c76e4703b9a4f8aa376035af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63f77b603dd343fb8709b5450293f42e",
            "placeholder": "​",
            "style": "IPY_MODEL_2b070f7e0fa14ac6b0f8adce0b83f4f0",
            "value": " 1.38k/1.38k [00:00&lt;00:00, 42.2kB/s]"
          }
        },
        "47fd626db1b643b08d5d0ed67de2c2c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "610de9413f2f41c299e140b5736afc69": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad8ffacca2984688aa2ec376fcded94a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "354d353579764f198407324b71411615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "394092ce70cc47f3a2a6a6cf894a95dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63f77b603dd343fb8709b5450293f42e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b070f7e0fa14ac6b0f8adce0b83f4f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e7dc6a7f11c4c38bbb674b5ace6b42e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9ecfe9ac3754409b937307cca9768332",
              "IPY_MODEL_e284acf04dc840ff826a074c5642c019",
              "IPY_MODEL_f4fbbc50f5eb4cd28b9a69f875d5761c"
            ],
            "layout": "IPY_MODEL_c41f25ee42314d56b702629ec2e68a0b"
          }
        },
        "9ecfe9ac3754409b937307cca9768332": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d54d898bdd4a4b79a8f5f151463db229",
            "placeholder": "​",
            "style": "IPY_MODEL_946aaddc22af4bd7ad1b70d17812a618",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "e284acf04dc840ff826a074c5642c019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08779aafb3674de3ba78cbfc4b61d6bd",
            "max": 308863317,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c371836ee16409f8702f2c6864616ef",
            "value": 308863317
          }
        },
        "f4fbbc50f5eb4cd28b9a69f875d5761c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42a7b27b005c47819d838e2e96901164",
            "placeholder": "​",
            "style": "IPY_MODEL_5b63079ef710400a88a3da181010282d",
            "value": " 309M/309M [00:04&lt;00:00, 110MB/s]"
          }
        },
        "c41f25ee42314d56b702629ec2e68a0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d54d898bdd4a4b79a8f5f151463db229": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "946aaddc22af4bd7ad1b70d17812a618": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "08779aafb3674de3ba78cbfc4b61d6bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c371836ee16409f8702f2c6864616ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "42a7b27b005c47819d838e2e96901164": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b63079ef710400a88a3da181010282d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Effect of Translation on ToM-like capabilities in Large Language Models\n",
        "\n",
        "\n",
        "*Author: Gayathri Anil*  \n",
        "*Last updated: 04/24/2023*  \n",
        "*References: https://osf.io/csdhb/* \n",
        "\n",
        "This notebook implements the pipeline to study the effect of Translation in ToM-like capabilities in Large Language Models like GPT-3"
      ],
      "metadata": {
        "id": "4RD7JMBKz92z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Environment Setup"
      ],
      "metadata": {
        "id": "CkRS4EUT1DNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U deep-translator\n",
        "!pip install -U easynmt\n",
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_d9v964z8HF",
        "outputId": "33dc2255-10e4-4a8a-db96-72c05b1d66f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting deep-translator\n",
            "  Downloading deep_translator-1.10.1-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.23.0 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (2.27.1)\n",
            "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from deep-translator) (4.11.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.0.12)\n",
            "Installing collected packages: deep-translator\n",
            "Successfully installed deep-translator-1.10.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting easynmt\n",
            "  Downloading EasyNMT-2.0.2.tar.gz (23 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from easynmt) (4.65.0)\n",
            "Collecting transformers<5,>=4.4\n",
            "  Downloading transformers-4.28.1-py3-none-any.whl (7.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from easynmt) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from easynmt) (1.22.4)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from easynmt) (3.8.1)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fasttext\n",
            "  Downloading fasttext-0.9.2.tar.gz (68 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.8/68.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from easynmt) (3.20.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->easynmt) (3.12.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->easynmt) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->easynmt) (3.1.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->easynmt) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->easynmt) (4.5.0)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->easynmt) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->easynmt) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->easynmt) (16.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.4->easynmt) (2022.10.31)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.14.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.4->easynmt) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.4->easynmt) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers<5,>=4.4->easynmt) (23.1)\n",
            "Collecting pybind11>=2.2\n",
            "  Using cached pybind11-2.10.4-py3-none-any.whl (222 kB)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from fasttext->easynmt) (67.7.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->easynmt) (8.1.3)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->easynmt) (1.2.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers<5,>=4.4->easynmt) (2023.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->easynmt) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=4.4->easynmt) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=4.4->easynmt) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers<5,>=4.4->easynmt) (1.26.15)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->easynmt) (1.3.0)\n",
            "Building wheels for collected packages: easynmt, fasttext\n",
            "  Building wheel for easynmt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for easynmt: filename=EasyNMT-2.0.2-py3-none-any.whl size=19920 sha256=78d649986281981fd82d505dd5ad495b8099a345f1bd5009f275b7c8ee5b368c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ab/55/72/aba4face7eac1d7750ca700aa1797b135fb8915e949da504cc\n",
            "  Building wheel for fasttext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fasttext: filename=fasttext-0.9.2-cp310-cp310-linux_x86_64.whl size=4393365 sha256=4c9ef743143e1b7483570eb885daefd51374007f73eb08d6d6989f699ad8d78b\n",
            "  Stored in directory: /root/.cache/pip/wheels/a5/13/75/f811c84a8ab36eedbaef977a6a58a98990e8e0f1967f98f394\n",
            "Successfully built easynmt fasttext\n",
            "Installing collected packages: tokenizers, sentencepiece, pybind11, huggingface-hub, fasttext, transformers, easynmt\n",
            "Successfully installed easynmt-2.0.2 fasttext-0.9.2 huggingface-hub-0.14.1 pybind11-2.10.4 sentencepiece-0.1.99 tokenizers-0.13.3 transformers-4.28.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting openai\n",
            "  Downloading openai-0.27.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai) (4.65.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai) (2.27.1)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai) (2022.12.7)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai) (23.1.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, async-timeout, yarl, aiosignal, aiohttp, openai\n",
            "Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 frozenlist-1.3.3 multidict-6.0.4 openai-0.27.6 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoU4pwRqzdLz"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai\n",
        "import numpy as np\n",
        "from random import shuffle\n",
        "import pandas as pd\n",
        "import requests\n",
        "from deep_translator import GoogleTranslator\n",
        "from easynmt import EasyNMT\n",
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# API Keys \n",
        "\n",
        "#openai_key = \"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\" #shared account\n",
        "#openai_key = \"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\" #personal account - new key\n",
        "openai_key = \"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\" #lina.gayathri\n",
        "hugging_face_key = \"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\"\n",
        "openai.api_key = openai_key"
      ],
      "metadata": {
        "id": "TEQCfSJ71X1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### User-defined Function Definitions\n",
        "\n"
      ],
      "metadata": {
        "id": "S6JH-BZ-1IoX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to query GPT-3 via API\n",
        "\n",
        "def gpt3(prompt, engine=\"text-davinci-003\", logprobs=1, echo_probs=False, echo_response=False, temperature=0, max_tokens = 10, **kwargs):\n",
        "  out=openai.Completion.create(prompt=prompt, engine=engine, logprobs=logprobs, temperature=temperature, max_tokens = max_tokens, **kwargs)\n",
        "  if echo_probs and logprobs>0:\n",
        "     x=out.choices[0][\"logprobs\"][\"top_logprobs\"][0]\n",
        "     print(\"Completion probabilities: \")\n",
        "     for i in x.keys(): print(i, round(np.exp(x[i]),2))\n",
        "  if echo_response:\n",
        "     print(\"Completion: \")\n",
        "     print(out.choices[0][\"text\"])\n",
        "  return([out.choices[0][\"text\"].strip(),round(np.exp(out.choices[0][\"logprobs\"][\"token_logprobs\"][0]),2)])"
      ],
      "metadata": {
        "id": "bqE7OSNBz80R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to query GPT-2 via API\n",
        "def hf(prompt, engine=\"gpt2\", temperature=.0001,max_tokens=50):\n",
        "    query ={\"inputs\": prompt, \n",
        "            \"parameters\": {\"temperature\": temperature,\"do_sample\":False, \"max_new_tokens\": max_tokens, \"max_time\": 120},\n",
        "            \"options\":{\"wait_for_model\":True}\n",
        "            }\n",
        "    API_URL = \"https://api-inference.huggingface.co/models/\"+engine\n",
        "    headers = {\"Authorization\": \"Bearer \"+hugging_face_key}\n",
        "    response = requests.post(API_URL, headers=headers, json=query)\n",
        "    #print(response)\n",
        "    out = response.json()\n",
        "    out = out[0][\"generated_text\"][len(prompt):]\n",
        "    return(out)"
      ],
      "metadata": {
        "id": "fFRWo6y0L8e1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to query responses for task type 1\n",
        "\n",
        "def taskProcessor(tsk, eng = 'text-davinci-003', len_out = 3, task_type = \"unexpected_contents\"): \n",
        "\n",
        "  model = EasyNMT('opus-mt')\n",
        "\n",
        "  if task_type == \"unexpected_contents\":\n",
        "  \n",
        "    #------------------------------\n",
        "    # Unexpcted Contents - Translation step\n",
        "    #------------------------------\n",
        "\n",
        "    txt=tsk[\"txt\"].replace(\"S1\",tsk[\"o1\"]).replace(\"S2\",tsk[\"o2\"]).replace(\"CX\",tsk[\"c\"]).replace(\"XNAM\",tsk[\"xnam\"]).replace(\"XPRO\",tsk[\"xpro\"]).replace(\"POS_PRO\",tsk[\"pos_pro\"]).replace(\". h\",\". H\").replace(\". s\",\". S\")\n",
        "    rtxt = model.translate(txt, target_lang='ru')\n",
        "    stxt = model.translate(txt, target_lang='es')\n",
        "    mtxt = GoogleTranslator(source='en', target='ml').translate(txt)\n",
        "    htxt = GoogleTranslator(source='en', target='hi').translate(txt)\n",
        "\n",
        "    txtr=txt.replace(tsk[\"o1\"], \"X1\").replace(tsk[\"o2\"], tsk[\"o1\"]).replace(\"X1\",tsk[\"o2\"]) \n",
        "    rtxtr = model.translate(txtr, target_lang='ru')\n",
        "    stxtr = model.translate(txtr, target_lang='es')\n",
        "    mtxtr = GoogleTranslator(source='en', target='ml').translate(txtr)\n",
        "    htxtr = GoogleTranslator(source='en', target='hi').translate(txtr)\n",
        "\n",
        "    q1 = tsk[\"xpro\"].capitalize()+\" opens the \"+tsk[\"c\"]+\" and looks inside. \"+tsk[\"xpro\"].capitalize()+\" can clearly see that it is full of\"\n",
        "    #correct task #17\n",
        "    if tsk[\"NUMBER\"]==17: \n",
        "        q1 = tsk[\"xpro\"].capitalize()+\" puts the \"+tsk[\"c\"]+\" on and listens to it. \"+tsk[\"xpro\"].capitalize()+\" can clearly hear that it is full of\"\n",
        "\n",
        "    rq1 = model.translate(q1, target_lang='ru')\n",
        "    sq1 = model.translate(q1, target_lang='es')\n",
        "    mq1 = GoogleTranslator(source='en', target='ml').translate(q1)\n",
        "    hq1 = GoogleTranslator(source='en', target='hi').translate(q1)\n",
        "\n",
        "    q2 = tsk[\"xpro\"].capitalize()+\" believes that the \"+tsk[\"c\"]+\" is full of\"\n",
        "    rq2 = model.translate(q2, target_lang='ru')\n",
        "    sq2 = model.translate(q2, target_lang='es')\n",
        "    mq2 = GoogleTranslator(source='en', target='ml').translate(q2)\n",
        "    hq2 = GoogleTranslator(source='en', target='hi').translate(q2)\n",
        "\n",
        "    q3 = tsk[\"xpro\"].capitalize()+\" calls \"+tsk[\"pos_pro\"]+\" friend to tell them that \"+tsk[\"xpro\"]+\" has just found a \"+tsk[\"c\"]+\" full of\"\n",
        "    rq3 = model.translate(q3, target_lang='ru')\n",
        "    sq3 = model.translate(q3, target_lang='es')\n",
        "    mq3 = GoogleTranslator(source='en', target='ml').translate(q3)\n",
        "    hq3 = GoogleTranslator(source='en', target='hi').translate(q3)\n",
        "        \n",
        "\n",
        "    #------------------------------\n",
        "    # Unexpcted Contents - Expected responses \n",
        "    #------------------------------\n",
        "\n",
        "    # expected response in english\n",
        "    expected_response = tsk[\"o1\"] + \" - \" + tsk[\"o2\"] + \" - \" + tsk[\"o2\"]\n",
        "\n",
        "    # expected response in spanish\n",
        "    es_o1 = model.translate(tsk[\"o1\"], target_lang='es')\n",
        "    es_o2 = model.translate(tsk[\"o2\"], target_lang='es')\n",
        "\n",
        "    # expected response in russian\n",
        "    ru_o1 = model.translate(tsk[\"o1\"], target_lang='ru')\n",
        "    ru_o2 = model.translate(tsk[\"o2\"], target_lang='ru')\n",
        "\n",
        "  elif task_type == \"unexpcted_transfer\":\n",
        "    #------------------------------\n",
        "    # Unexpcted Transfer - Translation step\n",
        "    #------------------------------\n",
        "\n",
        "\n",
        "    txt = tsk[\"txt\"]\n",
        "    rtxt = model.translate(txt, target_lang='ru')\n",
        "    stxt = model.translate(txt, target_lang='es')\n",
        "    mtxt = GoogleTranslator(source='en', target='ml').translate(txt)\n",
        "    htxt = GoogleTranslator(source='en', target='hi').translate(txt)\n",
        "\n",
        "    txtr=txt.replace(tsk[\"o1\"], \"PLACEHOLDER_FOR_O1\").replace(tsk[\"o2\"], tsk[\"o1\"]).replace(\"PLACEHOLDER_FOR_O1\", tsk[\"o2\"])\n",
        "    rtxtr = model.translate(txtr, target_lang='ru')\n",
        "    stxtr = model.translate(txtr, target_lang='es')\n",
        "    mtxtr = GoogleTranslator(source='en', target='ml').translate(txtr)\n",
        "    htxtr = GoogleTranslator(source='en', target='hi').translate(txtr)\n",
        "\n",
        "    q1 = tsk[\"q1\"]\n",
        "    rq1 = model.translate(q1, target_lang='ru')\n",
        "    sq1 = model.translate(q1, target_lang='es')\n",
        "    mq1 = GoogleTranslator(source='en', target='ml').translate(q1)\n",
        "    hq1 = GoogleTranslator(source='en', target='hi').translate(q1)\n",
        "\n",
        "\n",
        "    q2 = tsk[\"q2\"]\n",
        "    rq2 = model.translate(q2, target_lang='ru')\n",
        "    sq2 = model.translate(q2, target_lang='es')\n",
        "    mq2 = GoogleTranslator(source='en', target='ml').translate(q2)\n",
        "    hq2 = GoogleTranslator(source='en', target='hi').translate(q2)\n",
        "\n",
        "    q3 = tsk[\"q3\"]\n",
        "    rq3 = model.translate(q3, target_lang='ru')\n",
        "    sq3 = model.translate(q3, target_lang='es')\n",
        "    mq3 = GoogleTranslator(source='en', target='ml').translate(q3)\n",
        "    hq3 = GoogleTranslator(source='en', target='hi').translate(q3)\n",
        "        \n",
        "\n",
        "    #------------------------------\n",
        "    # Unexpcted Transfer - Expected responses \n",
        "    #------------------------------\n",
        "\n",
        "    # expected response in english\n",
        "    expected_response = tsk[\"o1\"] + \" - \" + tsk[\"o2\"] + \" - \" + tsk[\"o2\"]\n",
        "\n",
        "    # expected response in spanish\n",
        "    es_o1 = model.translate(tsk[\"o1\"], target_lang='es')\n",
        "    es_o2 = model.translate(tsk[\"o2\"], target_lang='es')\n",
        "\n",
        "    # expected response in russian\n",
        "    ru_o1 = model.translate(tsk[\"o1\"], target_lang='ru')\n",
        "    ru_o2 = model.translate(tsk[\"o2\"], target_lang='ru')\n",
        "\n",
        "\n",
        "  # Mic check\n",
        "  print(\"================================= TASK #\"+str(tsk[\"NUMBER\"])+ \" ================================= \")\n",
        "  print(\"Task (English): \"+txt)\n",
        "  print(\"Reverse Task (English): \"+txtr)\n",
        "  print(\"Prompt 1.1 (English): \"+q1+\" ______\")\n",
        "  print(\"Prompt 1.2 (English): \"+q2+\" ______\")\n",
        "  print(\"Prompt 1.3 (English): \"+q3+\" ______\")\n",
        "  print(\"=========================================================================== \")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  #------------------------------\n",
        "  # Task Response - Querying GPT2\n",
        "  #------------------------------\n",
        "\n",
        "  eng = \"gpt2\"\n",
        "\n",
        "  # English - English\n",
        "  e_e_res1 = hf(engine=eng, prompt=txt+q1, max_tokens=len_out, temperature=.0001)\n",
        "  e_e_res2 = hf(engine=eng, prompt=txt+q2, max_tokens=len_out, temperature=.0001)\n",
        "  e_e_res3 = hf(engine=eng, prompt=txt+q3, max_tokens=len_out, temperature=.0001)\n",
        "  e_e_res4 = hf(engine=eng, prompt=txtr+q1, max_tokens=len_out, temperature=.0001)\n",
        "  e_e_res5 = hf(engine=eng, prompt=txtr+q2, max_tokens=len_out, temperature=.0001)\n",
        "  e_e_res6 = hf(engine=eng, prompt=txtr+q3, max_tokens=len_out, temperature=.0001)\n",
        "\n",
        "  # English - Spanish\n",
        "  e_s_res1 = hf(engine=eng, prompt='Respond in Spanish. '+txt+q1, max_tokens=8, temperature=.0001)\n",
        "  e_s_res2 = hf(engine=eng, prompt='Respond in Spanish. '+txt+q2, max_tokens=8, temperature=.0001)\n",
        "  e_s_res3 = hf(engine=eng, prompt='Respond in Spanish. '+txt+q3, max_tokens=8, temperature=.0001)\n",
        "  e_s_res4 = hf(engine=eng, prompt='Respond in Spanish. '+txtr+q1, max_tokens=8, temperature=.0001)\n",
        "  e_s_res5 = hf(engine=eng, prompt='Respond in Spanish. '+txtr+q2, max_tokens=8, temperature=.0001)\n",
        "  e_s_res6 = hf(engine=eng, prompt='Respond in Spanish. '+txtr+q3, max_tokens=8, temperature=.0001)\n",
        "\n",
        "\n",
        "  # English - Russian\n",
        "  e_r_res1 = hf(engine=eng, prompt='Respond in Russian. '+txt+q1, max_tokens=8, temperature=.0001)\n",
        "  e_r_res2 = hf(engine=eng, prompt='Respond in Russian. '+txt+q2, max_tokens=8, temperature=.0001)\n",
        "  e_r_res3 = hf(engine=eng, prompt='Respond in Russian. '+txt+q3, max_tokens=8, temperature=.0001)\n",
        "  e_r_res4 = hf(engine=eng, prompt='Respond in Russian. '+txtr+q1, max_tokens=8, temperature=.0001)\n",
        "  e_r_res5 = hf(engine=eng, prompt='Respond in Russian. '+txtr+q2, max_tokens=8, temperature=.0001)\n",
        "  e_r_res6 = hf(engine=eng, prompt='Respond in Russian. '+txtr+q3, max_tokens=8, temperature=.0001)\n",
        "\n",
        "\n",
        "  # Spanish - English\n",
        "  s_e_res1 = hf(engine=eng, prompt='responder en ingles. '+stxt+sq1, max_tokens=len_out, temperature=.0001)\n",
        "  s_e_res2 = hf(engine=eng, prompt='responder en ingles. '+stxt+sq2, max_tokens=len_out, temperature=.0001)\n",
        "  s_e_res3 = hf(engine=eng, prompt='responder en ingles. '+stxt+sq3, max_tokens=len_out, temperature=.0001)\n",
        "  s_e_res4 = hf(engine=eng, prompt='responder en ingles. '+stxtr+sq1, max_tokens=len_out, temperature=.0001)\n",
        "  s_e_res5 = hf(engine=eng, prompt='responder en ingles. '+stxtr+sq2, max_tokens=len_out, temperature=.0001)\n",
        "  s_e_res6 = hf(engine=eng, prompt='responder en ingles. '+stxtr+sq3, max_tokens=len_out, temperature=.0001)\n",
        "\n",
        "\n",
        "  # Russian - English\n",
        "  r_e_res1 = hf(engine=eng, prompt='ответить на английском. '+rtxt+rq1, max_tokens=len_out, temperature=.0001)\n",
        "  r_e_res2 = hf(engine=eng, prompt='ответить на английском. '+rtxt+rq2, max_tokens=len_out, temperature=.0001)\n",
        "  r_e_res3 = hf(engine=eng, prompt='ответить на английском. '+rtxt+rq3, max_tokens=len_out, temperature=.0001)\n",
        "  r_e_res4 = hf(engine=eng, prompt='ответить на английском. '+rtxtr+rq1, max_tokens=len_out, temperature=.0001)\n",
        "  r_e_res5 = hf(engine=eng, prompt='ответить на английском. '+rtxtr+rq2, max_tokens=len_out, temperature=.0001)\n",
        "  r_e_res6 = hf(engine=eng, prompt='ответить на английском. '+rtxtr+rq3, max_tokens=len_out, temperature=.0001)\n",
        "\n",
        "\n",
        "  # Spanish - Spanish\n",
        "  s_s_res1 = hf(engine=eng, prompt=stxt+sq1, max_tokens=8, temperature=.0001)\n",
        "  s_s_res2 = hf(engine=eng, prompt=stxt+sq2, max_tokens=8, temperature=.0001)\n",
        "  s_s_res3 = hf(engine=eng, prompt=stxt+sq3, max_tokens=8, temperature=.0001)\n",
        "  s_s_res4 = hf(engine=eng, prompt=stxtr+sq1, max_tokens=8, temperature=.0001)\n",
        "  s_s_res5 = hf(engine=eng, prompt=stxtr+sq2, max_tokens=8, temperature=.0001)\n",
        "  s_s_res6 = hf(engine=eng, prompt=stxtr+sq3, max_tokens=8, temperature=.0001)\n",
        "\n",
        "  # Russian - Russian\n",
        "  r_r_res1 = hf(engine=eng, prompt=rtxt+rq1, max_tokens=8, temperature=.0001)\n",
        "  r_r_res2 = hf(engine=eng, prompt=rtxt+rq2, max_tokens=8, temperature=.0001)\n",
        "  r_r_res3 = hf(engine=eng, prompt=rtxt+rq3, max_tokens=8, temperature=.0001)\n",
        "  r_r_res4 = hf(engine=eng, prompt=rtxtr+rq1, max_tokens=8, temperature=.0001)\n",
        "  r_r_res5 = hf(engine=eng, prompt=rtxtr+rq2, max_tokens=8, temperature=.0001)\n",
        "  r_r_res6 = hf(engine=eng, prompt=rtxtr+rq3, max_tokens=8, temperature=.0001)\n",
        "\n",
        "  output_gpt2 = [\n",
        "      {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "       \"eng\": \"gpt2\",\n",
        "       \"prompt\": txt+q1,\n",
        "       \"eng_expected_response\": tsk[\"o1\"],\n",
        "       \"es_expected_response\": es_o1,\n",
        "       \"ru_expected_respnse\": ru_o1,\n",
        "       \"eng_eng_response\": e_e_res1,\n",
        "       \"eng_es_response\": e_s_res1,\n",
        "       \"eng_ru_response\": e_r_res1,\n",
        "       \"es_eng_response\": s_e_res1,\n",
        "       \"ru_eng_response\": r_e_res1,\n",
        "       \"es_es_response\": s_s_res1,\n",
        "       \"ru_ru_response\": r_r_res1,\n",
        "       \"es_prompt\": stxt+sq1,\n",
        "       \"ru_prompt\": rtxt+rq1\n",
        "       },\n",
        "       {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "        \"eng\": \"gpt2\",\n",
        "       \"prompt\": txt+q2,\n",
        "       \"eng_expected_response\": tsk[\"o2\"],\n",
        "       \"es_expected_response\": es_o2,\n",
        "       \"ru_expected_respnse\": ru_o2,\n",
        "       \"eng_eng_response\": e_e_res2,\n",
        "       \"eng_es_response\": e_s_res2,\n",
        "       \"eng_ru_response\": e_r_res2,\n",
        "       \"es_eng_response\": s_e_res2,\n",
        "       \"ru_eng_response\": r_e_res2,\n",
        "       \"es_es_response\": s_s_res2,\n",
        "       \"ru_ru_response\": r_r_res2,\n",
        "       \"es_prompt\": stxt+sq2,\n",
        "       \"ru_prompt\": rtxt+rq2\n",
        "       },\n",
        "       {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "        \"eng\": \"gpt2\",\n",
        "       \"prompt\": txt+q3,\n",
        "       \"eng_expected_response\": tsk[\"o2\"],\n",
        "       \"es_expected_response\": es_o2,\n",
        "       \"ru_expected_respnse\": ru_o2,\n",
        "       \"eng_eng_response\": e_e_res3,\n",
        "       \"eng_es_response\": e_s_res3,\n",
        "       \"eng_ru_response\": e_r_res3,\n",
        "       \"es_eng_response\": s_e_res3,\n",
        "       \"ru_eng_response\": r_e_res3,\n",
        "       \"es_es_response\": s_s_res3,\n",
        "       \"ru_ru_response\": r_r_res3,\n",
        "       \"es_prompt\": stxt+sq3,\n",
        "       \"ru_prompt\": rtxt+rq3\n",
        "       },\n",
        "       {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "        \"eng\": \"gpt2\",\n",
        "       \"prompt\": txtr+q1,\n",
        "       \"eng_expected_response\": tsk[\"o2\"],\n",
        "       \"es_expected_response\": es_o2,\n",
        "       \"ru_expected_respnse\": ru_o2,\n",
        "       \"eng_eng_response\": e_e_res4,\n",
        "       \"eng_es_response\": e_s_res4,\n",
        "       \"eng_ru_response\": e_r_res4,\n",
        "       \"es_eng_response\": s_e_res4,\n",
        "       \"ru_eng_response\": r_e_res4,\n",
        "       \"es_es_response\": s_s_res4,\n",
        "       \"ru_ru_response\": r_r_res4,\n",
        "       \"es_prompt\": stxtr+sq1,\n",
        "       \"ru_prompt\": rtxtr+rq1\n",
        "       },\n",
        "       {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "        \"eng\": \"gpt2\",\n",
        "       \"prompt\": txtr+q2,\n",
        "       \"eng_expected_response\": tsk[\"o1\"],\n",
        "       \"es_expected_response\": es_o1,\n",
        "       \"ru_expected_respnse\": ru_o1,\n",
        "       \"eng_eng_response\": e_e_res5,\n",
        "       \"eng_es_response\": e_s_res5,\n",
        "       \"eng_ru_response\": e_r_res5,\n",
        "       \"es_eng_response\": s_e_res5,\n",
        "       \"ru_eng_response\": r_e_res5,\n",
        "       \"es_es_response\": s_s_res5,\n",
        "       \"ru_ru_response\": r_r_res5,\n",
        "       \"es_prompt\": stxtr+sq2,\n",
        "       \"ru_prompt\": rtxtr+rq2\n",
        "       },\n",
        "       {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "        \"eng\": \"gpt2\",\n",
        "       \"prompt\": txtr+q3,\n",
        "       \"eng_expected_response\": tsk[\"o1\"],\n",
        "       \"es_expected_response\": es_o1,\n",
        "       \"ru_expected_respnse\": ru_o1,\n",
        "       \"eng_eng_response\": e_e_res6,\n",
        "       \"eng_es_response\": e_s_res6,\n",
        "       \"eng_ru_response\": e_r_res6,\n",
        "       \"es_eng_response\": s_e_res6,\n",
        "       \"ru_eng_response\": r_e_res6,\n",
        "       \"es_es_response\": s_s_res6,\n",
        "       \"ru_ru_response\": r_r_res6,\n",
        "       \"es_prompt\": stxtr+sq3,\n",
        "       \"ru_prompt\": rtxtr+rq3\n",
        "       }\n",
        "  ]\n",
        "\n",
        "  #------------------------------\n",
        "  # Task Response - Querying GPT3\n",
        "  #------------------------------\n",
        "\n",
        "  eng = 'text-davinci-003'\n",
        "\n",
        "  # English - English\n",
        "  e_e_res1 = gpt3(engine=eng, prompt=txt+q1, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_e_res2 = gpt3(engine=eng, prompt=txt+q2, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_e_res3 = gpt3(engine=eng, prompt=txt+q3, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_e_res4 = gpt3(engine=eng, prompt=txtr+q1, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_e_res5 = gpt3(engine=eng, prompt=txtr+q2, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_e_res6 = gpt3(engine=eng, prompt=txtr+q3, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "\n",
        "\n",
        "  # English - Spanish\n",
        "  e_s_res1 = gpt3(engine=eng, prompt='Respond in Spanish. '+txt+q1, max_tokens=8, temperature=0, logprobs=2)\n",
        "  e_s_res2 = gpt3(engine=eng, prompt='Respond in Spanish. '+txt+q2, max_tokens=8, temperature=0, logprobs=2)\n",
        "  e_s_res3 = gpt3(engine=eng, prompt='Respond in Spanish. '+txt+q3, max_tokens=8, temperature=0, logprobs=2)\n",
        "  e_s_res4 = gpt3(engine=eng, prompt='Respond in Spanish. '+txtr+q1, max_tokens=8, temperature=0, logprobs=2)\n",
        "  e_s_res5 = gpt3(engine=eng, prompt='Respond in Spanish. '+txtr+q2, max_tokens=8, temperature=0, logprobs=2)\n",
        "  e_s_res6 = gpt3(engine=eng, prompt='Respond in Spanish. '+txtr+q3, max_tokens=8, temperature=0, logprobs=2)\n",
        "\n",
        "\n",
        "  # English - Russian\n",
        "  e_r_res1 = gpt3(engine=eng, prompt='Respond in Russian. '+txt+q1, max_tokens=8, temperature=0, logprobs=2)\n",
        "  e_r_res2 = gpt3(engine=eng, prompt='Respond in Russian. '+txt+q2, max_tokens=8, temperature=0, logprobs=2)\n",
        "  e_r_res3 = gpt3(engine=eng, prompt='Respond in Russian. '+txt+q3, max_tokens=8, temperature=0, logprobs=2)\n",
        "  e_r_res4 = gpt3(engine=eng, prompt='Respond in Russian. '+txtr+q1, max_tokens=8, temperature=0, logprobs=2)\n",
        "  e_r_res5 = gpt3(engine=eng, prompt='Respond in Russian. '+txtr+q2, max_tokens=8, temperature=0, logprobs=2)\n",
        "  e_r_res6 = gpt3(engine=eng, prompt='Respond in Russian. '+txtr+q3, max_tokens=8, temperature=0, logprobs=2)\n",
        "\n",
        "\n",
        "  # Spanish - English\n",
        "  s_e_res1 = gpt3(engine=eng, prompt='responder en ingles. '+stxt+sq1, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  s_e_res2 = gpt3(engine=eng, prompt='responder en ingles. '+stxt+sq2, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  s_e_res3 = gpt3(engine=eng, prompt='responder en ingles. '+stxt+sq3, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  s_e_res4 = gpt3(engine=eng, prompt='responder en ingles. '+stxtr+sq1, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  s_e_res5 = gpt3(engine=eng, prompt='responder en ingles. '+stxtr+sq2, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  s_e_res6 = gpt3(engine=eng, prompt='responder en ingles. '+stxtr+sq3, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "\n",
        "\n",
        "  # Russian - English\n",
        "  r_e_res1 = gpt3(engine=eng, prompt='ответить на английском. '+rtxt+rq1, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  r_e_res2 = gpt3(engine=eng, prompt='ответить на английском. '+rtxt+rq2, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  r_e_res3 = gpt3(engine=eng, prompt='ответить на английском. '+rtxt+rq3, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  r_e_res4 = gpt3(engine=eng, prompt='ответить на английском. '+rtxtr+rq1, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  r_e_res5 = gpt3(engine=eng, prompt='ответить на английском. '+rtxtr+rq2, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  r_e_res6 = gpt3(engine=eng, prompt='ответить на английском. '+rtxtr+rq3, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "\n",
        "\n",
        "  # Spanish - Spanish\n",
        "  s_s_res1 = gpt3(engine=eng, prompt=stxt+sq1, max_tokens=8, temperature=0, logprobs=2)\n",
        "  s_s_res2 = gpt3(engine=eng, prompt=stxt+sq2, max_tokens=8, temperature=0, logprobs=2)\n",
        "  s_s_res3 = gpt3(engine=eng, prompt=stxt+sq3, max_tokens=8, temperature=0, logprobs=2)\n",
        "  s_s_res4 = gpt3(engine=eng, prompt=stxtr+sq1, max_tokens=8, temperature=0, logprobs=2)\n",
        "  s_s_res5 = gpt3(engine=eng, prompt=stxtr+sq2, max_tokens=8, temperature=0, logprobs=2)\n",
        "  s_s_res6 = gpt3(engine=eng, prompt=stxtr+sq3, max_tokens=8, temperature=0, logprobs=2)\n",
        "\n",
        "  # Russian - Russian\n",
        "  r_r_res1 = gpt3(engine=eng, prompt=rtxt+rq1, max_tokens=8, temperature=0, logprobs=2)\n",
        "  r_r_res2 = gpt3(engine=eng, prompt=rtxt+rq2, max_tokens=8, temperature=0, logprobs=2)\n",
        "  r_r_res3 = gpt3(engine=eng, prompt=rtxt+rq3, max_tokens=8, temperature=0, logprobs=2)\n",
        "  r_r_res4 = gpt3(engine=eng, prompt=rtxtr+rq1, max_tokens=8, temperature=0, logprobs=2)\n",
        "  r_r_res5 = gpt3(engine=eng, prompt=rtxtr+rq2, max_tokens=8, temperature=0, logprobs=2)\n",
        "  r_r_res6 = gpt3(engine=eng, prompt=rtxtr+rq3, max_tokens=8, temperature=0, logprobs=2)\n",
        "\n",
        "  output_gpt3 = [\n",
        "      {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "       \"eng\": \"gpt3\",\n",
        "       \"prompt\": txt+q1,\n",
        "       \"eng_expected_response\": tsk[\"o1\"],\n",
        "       \"es_expected_response\": es_o1,\n",
        "       \"ru_expected_respnse\": ru_o1,\n",
        "       \"eng_eng_response\": e_e_res1[0],\n",
        "       \"eng_es_response\": e_s_res1[0],\n",
        "       \"eng_ru_response\": e_r_res1[0],\n",
        "       \"es_eng_response\": s_e_res1[0],\n",
        "       \"ru_eng_response\": r_e_res1[0],\n",
        "       \"es_es_response\": s_s_res1[0],\n",
        "       \"ru_ru_response\": r_r_res1[0],\n",
        "       \"es_prompt\": stxt+sq1,\n",
        "       \"ru_prompt\": rtxt+rq1\n",
        "       },\n",
        "       {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "       \"eng\": \"gpt3\",\n",
        "       \"prompt\": txt+q2,\n",
        "       \"eng_expected_response\": tsk[\"o2\"],\n",
        "       \"es_expected_response\": es_o2,\n",
        "       \"ru_expected_respnse\": ru_o2,\n",
        "       \"eng_eng_response\": e_e_res2[0],\n",
        "       \"eng_es_response\": e_s_res2[0],\n",
        "       \"eng_ru_response\": e_r_res2[0],\n",
        "       \"es_eng_response\": s_e_res2[0],\n",
        "       \"ru_eng_response\": r_e_res2[0],\n",
        "       \"es_es_response\": s_s_res2[0],\n",
        "       \"ru_ru_response\": r_r_res2[0],\n",
        "       \"es_prompt\": stxt+sq2,\n",
        "       \"ru_prompt\": rtxt+rq2\n",
        "       },\n",
        "       {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "       \"eng\": \"gpt3\",\n",
        "       \"prompt\": txt+q3,\n",
        "       \"eng_expected_response\": tsk[\"o2\"],\n",
        "       \"es_expected_response\": es_o2,\n",
        "       \"ru_expected_respnse\": ru_o2,\n",
        "       \"eng_eng_response\": e_e_res3[0],\n",
        "       \"eng_es_response\": e_s_res3[0],\n",
        "       \"eng_ru_response\": e_r_res3[0],\n",
        "       \"es_eng_response\": s_e_res3[0],\n",
        "       \"ru_eng_response\": r_e_res3[0],\n",
        "       \"es_es_response\": s_s_res3[0],\n",
        "       \"ru_ru_response\": r_r_res3[0],\n",
        "       \"es_prompt\": stxt+sq3,\n",
        "       \"ru_prompt\": rtxt+rq3\n",
        "       },\n",
        "       {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "       \"eng\": \"gpt3\",\n",
        "       \"prompt\": txtr+q1,\n",
        "       \"eng_expected_response\": tsk[\"o2\"],\n",
        "       \"es_expected_response\": es_o2,\n",
        "       \"ru_expected_respnse\": ru_o2,\n",
        "       \"eng_eng_response\": e_e_res4[0],\n",
        "       \"eng_es_response\": e_s_res4[0],\n",
        "       \"eng_ru_response\": e_r_res4[0],\n",
        "       \"es_eng_response\": s_e_res4[0],\n",
        "       \"ru_eng_response\": r_e_res4[0],\n",
        "       \"es_es_response\": s_s_res4[0],\n",
        "       \"ru_ru_response\": r_r_res4[0],\n",
        "       \"es_prompt\": stxtr+sq1,\n",
        "       \"ru_prompt\": rtxtr+rq1\n",
        "       },\n",
        "       {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "       \"eng\": \"gpt3\",\n",
        "       \"prompt\": txtr+q2,\n",
        "       \"eng_expected_response\": tsk[\"o1\"],\n",
        "       \"es_expected_response\": es_o1,\n",
        "       \"ru_expected_respnse\": ru_o1,\n",
        "       \"eng_eng_response\": e_e_res5[0],\n",
        "       \"eng_es_response\": e_s_res5[0],\n",
        "       \"eng_ru_response\": e_r_res5[0],\n",
        "       \"es_eng_response\": s_e_res5[0],\n",
        "       \"ru_eng_response\": r_e_res5[0],\n",
        "       \"es_es_response\": s_s_res5[0],\n",
        "       \"ru_ru_response\": r_r_res5[0],\n",
        "       \"es_prompt\": stxtr+sq2,\n",
        "       \"ru_prompt\": rtxtr+rq2\n",
        "       },\n",
        "       {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "       \"eng\": \"gpt3\",\n",
        "       \"prompt\": txtr+q3,\n",
        "       \"eng_expected_response\": tsk[\"o1\"],\n",
        "       \"es_expected_response\": es_o1,\n",
        "       \"ru_expected_respnse\": ru_o1,\n",
        "       \"eng_eng_response\": e_e_res6[0],\n",
        "       \"eng_es_response\": e_s_res6[0],\n",
        "       \"eng_ru_response\": e_r_res6[0],\n",
        "       \"es_eng_response\": s_e_res6[0],\n",
        "       \"ru_eng_response\": r_e_res6[0],\n",
        "       \"es_es_response\": s_s_res6[0],\n",
        "       \"ru_ru_response\": r_r_res6[0],\n",
        "       \"es_prompt\": stxtr+sq3,\n",
        "       \"ru_prompt\": rtxtr+rq3\n",
        "       }\n",
        "  ]\n",
        "\n",
        "  return output_gpt2, output_gpt3\n",
        "\n",
        "\n",
        "          \n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mauqGVJu1tU0"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "# Function to query responses for task type 2\n",
        "\n",
        "def unexpectedTransferTasks(tsk, eng = 'text-davinci-003', len_out = 3):\n",
        "\n",
        "  #------------------------------\n",
        "  # Translation step\n",
        "  #------------------------------\n",
        "\n",
        "  model = EasyNMT('opus-mt')\n",
        "\n",
        "  txt = tsk[\"txt\"]\n",
        "  rtxt = model.translate(txt, target_lang='ru')\n",
        "  stxt = model.translate(txt, target_lang='es')\n",
        "  mtxt = GoogleTranslator(source='en', target='ml').translate(txt)\n",
        "  htxt = GoogleTranslator(source='en', target='hi').translate(txt)\n",
        "\n",
        "  txtr=txt.replace(tsk[\"o1\"], \"PLACEHOLDER_FOR_O1\").replace(tsk[\"o2\"], tsk[\"o1\"]).replace(\"PLACEHOLDER_FOR_O1\", tsk[\"o2\"])\n",
        "  rtxtr = model.translate(txtr, target_lang='ru')\n",
        "  stxtr = model.translate(txtr, target_lang='es')\n",
        "  mtxtr = GoogleTranslator(source='en', target='ml').translate(txtr)\n",
        "  htxtr = GoogleTranslator(source='en', target='hi').translate(txtr)\n",
        "\n",
        "  q1 = tsk[\"q1\"]\n",
        "  rq1 = model.translate(q1, target_lang='ru')\n",
        "  sq1 = model.translate(q1, target_lang='es')\n",
        "  mq1 = GoogleTranslator(source='en', target='ml').translate(q1)\n",
        "  hq1 = GoogleTranslator(source='en', target='hi').translate(q1)\n",
        "\n",
        "\n",
        "  q2 = tsk[\"q2\"]\n",
        "  rq2 = model.translate(q2, target_lang='ru')\n",
        "  sq2 = model.translate(q2, target_lang='es')\n",
        "  mq2 = GoogleTranslator(source='en', target='ml').translate(q2)\n",
        "  hq2 = GoogleTranslator(source='en', target='hi').translate(q2)\n",
        "\n",
        "  q3 = tsk[\"q3\"]\n",
        "  rq3 = model.translate(q3, target_lang='ru')\n",
        "  sq3 = model.translate(q3, target_lang='es')\n",
        "  mq3 = GoogleTranslator(source='en', target='ml').translate(q3)\n",
        "  hq3 = GoogleTranslator(source='en', target='hi').translate(q3)\n",
        "      \n",
        "\n",
        "  print(\"================================= TASK #\"+str(tsk[\"NUMBER\"])+ \" ================================= \")\n",
        "  print(\"TASK (English): \"+txt)\n",
        "  print(\"TASK (Russian): \"+rtxt)\n",
        "  print(\"TASK (Spanish): \"+stxt)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"REVERSED TASK (English): \"+txtr)\n",
        "  print(\"REVERSED TASK (Russian): \"+rtxtr)\n",
        "  print(\"REVERSED TASK (Spanish): \"+stxtr)\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"PROMPT 1.1 (English): \"+q1+\" ______\")\n",
        "  print(\"PROMPT 1.1 (Russian): \"+rq1+\" ______\")\n",
        "  print(\"PROMPT 1.1 (Spanish): \"+sq1+\" ______\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"PROMPT 1.2 (English): \"+q2+\" ______\")\n",
        "  print(\"PROMPT 1.2 (Russian): \"+rq2+\" ______\")\n",
        "  print(\"PROMPT 1.2 (Spanish): \"+sq2+\" ______\")\n",
        "  print(\"\\n\")\n",
        "\n",
        "  print(\"PROMPT 1.3 (English): \"+q3+\" ______\")\n",
        "  print(\"PROMPT 1.3 (Russian): \"+rq3+\" ______\")\n",
        "  print(\"PROMPT 1.3 (Spanish): \"+sq3+\" ______\")\n",
        "  print(\"\\n\")\n",
        "  print(\"=========================================================================== \")\n",
        "  print(\"\\n \\n\")\n",
        "\n",
        "  #------------------------------\n",
        "  # Task Response Querying\n",
        "  #------------------------------\n",
        "\n",
        "  expected_response = tsk[\"o1\"] + \" - \" + tsk[\"o2\"] + \" - \" + tsk[\"o2\"]\n",
        "  es_expected_response = model.translate(expected_response, target_lang='es')\n",
        "  ru_expected_response = model.translate(expected_response, target_lang='ru')\n",
        "  print(\"## EXPECTED PATTERN: \"+ expected_response)\n",
        "  print(\"\\n\")\n",
        "\n",
        "\n",
        "  #------------------------------\n",
        "  # Task Response Querying\n",
        "  #------------------------------\n",
        "\n",
        "  # English - English\n",
        "  e_e_res1 = gpt3(engine=eng, prompt=txt+q1, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_e_res2 = gpt3(engine=eng, prompt=txt+q2, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_e_res3 = gpt3(engine=eng, prompt=txt+q3, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_e_response = e_e_res1[0] + \" - \" + e_e_res2[0] + \" - \" + e_e_res3[0]\n",
        "  print(\"English-English Response: \", e_e_response)\n",
        "\n",
        "  # English - Spanish\n",
        "  e_s_res1 = gpt3(engine=eng, prompt='Respond in Spanish. '+txt+q1, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_s_res2 = gpt3(engine=eng, prompt='Respond in Spanish. '+txt+q2, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_s_res3 = gpt3(engine=eng, prompt='Respond in Spanish. '+txt+q3, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_s_response = e_s_res1[0] + \" - \" + e_s_res2[0] + \" - \" + e_s_res3[0]\n",
        "  e_s_response_e = model.translate(e_s_response, source_lang = 'es', target_lang='en')\n",
        "  print(\"English-Spanish Response: \", e_s_response, \" --- \", e_s_response_e)\n",
        "\n",
        "  # English - Russian\n",
        "  e_r_res1 = gpt3(engine=eng, prompt='Respond in Russian. '+txt+q1, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_r_res2 = gpt3(engine=eng, prompt='Respond in Russian. '+txt+q2, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_r_res3 = gpt3(engine=eng, prompt='Respond in Russian. '+txt+q3, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  e_r_response = e_r_res1[0] + \" - \" + e_r_res2[0] + \" - \" + e_r_res3[0]\n",
        "  e_r_response_e = model.translate(e_r_response, source_lang = 'ru', target_lang='en')\n",
        "  print(\"English-Russian Response: \", e_r_response, \" --- \", e_r_response_e)\n",
        "\n",
        "  # Spanish - English\n",
        "  s_e_res1 = gpt3(engine=eng, prompt='responder en ingles. '+stxt+sq1, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  s_e_res2 = gpt3(engine=eng, prompt='responder en ingles. '+stxt+sq2, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  s_e_res3 = gpt3(engine=eng, prompt='responder en ingles. '+stxt+sq3, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  s_e_response = s_e_res1[0] + \" - \" + s_e_res2[0] + \" - \" + s_e_res3[0]\n",
        "  print(\"Spanish-English Response: \", s_e_response)\n",
        "\n",
        "  # Russian - English\n",
        "  r_e_res1 = gpt3(engine=eng, prompt='ответить на английском. '+rtxt+rq1, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  r_e_res2 = gpt3(engine=eng, prompt='ответить на английском. '+rtxt+rq2, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  r_e_res3 = gpt3(engine=eng, prompt='ответить на английском. '+rtxt+rq3, max_tokens=len_out, temperature=0, logprobs=2)\n",
        "  r_e_response = r_e_res1[0] + \" - \" + r_e_res2[0] + \" - \" + r_e_res3[0]\n",
        "  print(\"Russian-English Response: \", r_e_response)\n",
        "\n",
        "  output = {\"task#\": tsk[\"NUMBER\"]+1,\n",
        "            \"prompt1\": txt+q1,\n",
        "            \"prompt2\": txt+q2,\n",
        "            \"prompt3\": txt+q3,\n",
        "            \"expected-response\": expected_response,\n",
        "            \"es_expected_response\": es_expected_response,\n",
        "            \"ru_expected_response\": ru_expected_response,\n",
        "            \"eng-eng-response\": e_e_response,\n",
        "            \"eng-es-response\": e_s_response,\n",
        "            \"eng-es-eng-response\": e_s_response_e,\n",
        "            \"eng-rus-response\": e_r_response,\n",
        "            \"eng-rus-eng-response\": e_r_response_e,\n",
        "            \"es-eng-response\": s_e_response,\n",
        "            \"rus-eng-response\": r_e_response,\n",
        "            \"es-prompt1\": stxt+sq1,\n",
        "            \"es-prompt2\": stxt+sq2,\n",
        "            \"es-prompt3\": stxt+sq3,\n",
        "            \"ru-prompt1\": rtxt+rq1,\n",
        "            \"ru-prompt2\": rtxt+rq2,\n",
        "            \"ru-prompt3\": rtxt+rq3}\n",
        "\n",
        "  return output\n",
        "\n",
        "          \n",
        "\"\"\"\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NzhCFDg4Df0p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unexpected Contents Task \n",
        "\n",
        "### Task list\n",
        "\n"
      ],
      "metadata": {
        "id": "1LmULq1hRuHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsks1 = [\n",
        "    {\"txt\": r\"\"\"On the shelf, there is a CX. It is full of S1; there is no S2 in it. Yet, the label on this CX says \"S2\" and not \"S1\". XNAM walks into the room and notices the CX. XPRO has never seen it before. He reads the label. \"\"\",\n",
        "    \"o1\": \"beer\",     \"o2\": \"wine\", \"c\":\"bottle\", \"xnam\":\"Mark\", \"xpro\":\"he\", \"pos_pro\":\"his\", \"q3\":\"loves drinking\"}]\n",
        "j=0\n",
        "for i in tsks1:\n",
        "  i[\"NUMBER\"]=j\n",
        "  j=j+1\n",
        "\"\"\"\n",
        "tsk = tsks1[0]\n",
        "print(tsk)\n",
        "\n",
        "txt=tsk[\"txt\"].replace(\"S1\",tsk[\"o1\"]).replace(\"S2\",tsk[\"o2\"]).replace(\"CX\",tsk[\"c\"]).replace(\"XNAM\",tsk[\"xnam\"]).replace(\"XPRO\",tsk[\"xpro\"]).replace(\"POS_PRO\",tsk[\"pos_pro\"]).replace(\". h\",\". H\").replace(\". s\",\". S\")\n",
        "q1 = tsk[\"xpro\"].capitalize()+\" opens the \"+tsk[\"c\"]+\" and looks inside. \"+tsk[\"xpro\"].capitalize()+\" can clearly see that it is full of\"\n",
        "q2 = tsk[\"xpro\"].capitalize()+\" believes that the \"+tsk[\"c\"]+\" is full of\"\n",
        "q3 = tsk[\"xpro\"].capitalize()+\" calls \"+tsk[\"pos_pro\"]+\" friend to tell them that \"+tsk[\"xpro\"]+\" has just found a \"+tsk[\"c\"]+\" full of\"\n",
        "q4 = \"Before openining the \"+tsk[\"c\"]+\" \" +tsk[\"xpro\"]+\" believed that the \"+tsk[\"c\"]+\" is full of\"\n",
        "\n",
        "print(\"q1: \", q1, \"\\nq2: \", q2, \"\\nq3: \", q3, \"\\nq4: \", q4)\n",
        "expected_response = tsk[\"o1\"] + \" - \" + tsk[\"o2\"] + \" - \" + tsk[\"o2\"]\n",
        "print(\"expected_response: \", expected_response)\n",
        "\n",
        "e_e_res1 = gpt3(engine=\"text-davinci-003\", prompt=txt+q1, max_tokens=3, temperature=0, logprobs=2)\n",
        "e_e_res2 = gpt3(engine=\"text-davinci-003\", prompt=txt+q2, max_tokens=3, temperature=0, logprobs=2)\n",
        "e_e_res3 = gpt3(engine=\"text-davinci-003\", prompt=txt+q3, max_tokens=3, temperature=0, logprobs=2)\n",
        "e_e_res4 = gpt3(engine=\"text-davinci-003\", prompt=txt+q4, max_tokens=3, temperature=0, logprobs=2)\n",
        "print(\"1: \", e_e_res1)\n",
        "print(\"2: \", e_e_res2)\n",
        "print(\"3: \", e_e_res3)\n",
        "print(\"4: \", e_e_res4)\n",
        "\n",
        "#h = hf(engine=\"gpt2\", prompt=txt+q1, max_tokens=2,temperature=.0001)\n",
        "#print(\"h: \", h)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 125
        },
        "id": "cdhAlsu7N4Yf",
        "outputId": "38ae3cdd-32d9-4330-f6d2-27118a7aac92"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ntsk = tsks1[0]\\nprint(tsk)\\n\\ntxt=tsk[\"txt\"].replace(\"S1\",tsk[\"o1\"]).replace(\"S2\",tsk[\"o2\"]).replace(\"CX\",tsk[\"c\"]).replace(\"XNAM\",tsk[\"xnam\"]).replace(\"XPRO\",tsk[\"xpro\"]).replace(\"POS_PRO\",tsk[\"pos_pro\"]).replace(\". h\",\". H\").replace(\". s\",\". S\")\\nq1 = tsk[\"xpro\"].capitalize()+\" opens the \"+tsk[\"c\"]+\" and looks inside. \"+tsk[\"xpro\"].capitalize()+\" can clearly see that it is full of\"\\nq2 = tsk[\"xpro\"].capitalize()+\" believes that the \"+tsk[\"c\"]+\" is full of\"\\nq3 = tsk[\"xpro\"].capitalize()+\" calls \"+tsk[\"pos_pro\"]+\" friend to tell them that \"+tsk[\"xpro\"]+\" has just found a \"+tsk[\"c\"]+\" full of\"\\nq4 = \"Before openining the \"+tsk[\"c\"]+\" \" +tsk[\"xpro\"]+\" believed that the \"+tsk[\"c\"]+\" is full of\"\\n\\nprint(\"q1: \", q1, \"\\nq2: \", q2, \"\\nq3: \", q3, \"\\nq4: \", q4)\\nexpected_response = tsk[\"o1\"] + \" - \" + tsk[\"o2\"] + \" - \" + tsk[\"o2\"]\\nprint(\"expected_response: \", expected_response)\\n\\ne_e_res1 = gpt3(engine=\"text-davinci-003\", prompt=txt+q1, max_tokens=3, temperature=0, logprobs=2)\\ne_e_res2 = gpt3(engine=\"text-davinci-003\", prompt=txt+q2, max_tokens=3, temperature=0, logprobs=2)\\ne_e_res3 = gpt3(engine=\"text-davinci-003\", prompt=txt+q3, max_tokens=3, temperature=0, logprobs=2)\\ne_e_res4 = gpt3(engine=\"text-davinci-003\", prompt=txt+q4, max_tokens=3, temperature=0, logprobs=2)\\nprint(\"1: \", e_e_res1)\\nprint(\"2: \", e_e_res2)\\nprint(\"3: \", e_e_res3)\\nprint(\"4: \", e_e_res4)\\n\\n#h = hf(engine=\"gpt2\", prompt=txt+q1, max_tokens=2,temperature=.0001)\\n#print(\"h: \", h)\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "e_e_res1[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Zxdg5qPxb2tn",
        "outputId": "2ba163de-d61c-43e4-ca1e-a27ed0363dfa"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'beer,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# unexpected Contents tasks\n",
        "tsks1 = [\n",
        "    {\"txt\": r\"\"\"On the shelf, there is a CX. It is full of S1; there is no S2 in it. Yet, the label on this CX says \"S2\" and not \"S1\". XNAM walks into the room and notices the CX. XPRO has never seen it before. He reads the label. \"\"\",\n",
        "    \"o1\": \"beer\",     \"o2\": \"wine\", \"c\":\"bottle\", \"xnam\":\"Mark\", \"xpro\":\"he\", \"pos_pro\":\"his\", \"q3\":\"loves drinking\"}, \n",
        "    \n",
        "    {\"txt\": r\"\"\"On the shelf in the company's headquarters, there is a CX that contains only S1 files and no S2 files. Yet, confusingly, its label clearly states \"S2 files\" and not \"S1 files\". The newly hired computer engineer finds the CX on the shelf. XPRO has never seen this CX before. XPRO reads its label. \"\"\",\n",
        "    \"o1\": \"audio\", \"o2\": \"video\",\"c\":\"hard drive\", \"xnam\":\"computer engineer\", \"xpro\":\"she\", \"pos_pro\":\"her\", \"q3\":\"loves browsing through old files containing\"},   \n",
        "    \n",
        "    {\"txt\": r\"\"\"In the freezer, there is a CX filled with S1. There is no S2 in it. Yet, the label says \"S2\" and not \"S1\". The label is wrong. One day, XNAM finds the CX and realizes that XPRO has never seen it before. XPRO cannot see what is inside the CX. XPRO reads the label. \"\"\",\n",
        "    \"o1\": \"ice cream\",     \"o2\": \"jam\",\"c\":\"container\", \"xnam\":\"Anna\", \"xpro\":\"she\", \"pos_pro\":\"her\",\"q3\":\"loves eating\"},   \n",
        "\n",
        "    {\"txt\": r\"\"\"A CX full of S1 arrives at Amsterdam port. There is no S2 in it. It is sealed, and one cannot see what is inside. The shipping documents state that it contains S2. A customs officer approaches the CX. XPRO cannot see what is inside. XPRO reads the shipping documents to figure out what is inside. \"\"\",\n",
        "    \"o1\": \"salt\",     \"o2\": \"sugar\",\"c\":\"shipping container\", \"xnam\":\"customs officer\", \"xpro\":\"she\", \"pos_pro\":\"her\", \"q3\":\"thinks that it is much easier to inspect shipping containers filled with\"},   \n",
        "\n",
        "    {\"txt\": r\"\"\"There is a closed CX on the table of an empty meeting room, in which a seminar is about to take place. The CX is filled with S1, and there are no S2 in it. Yet, it is labeled with the word \"S2\". The first attendee walks into the room and sits in a chair in front of the CX. XPRO reads the label. \"\"\",\n",
        "    \"o1\": \"pens\",     \"o2\": \"pencils\",\"c\":\"box\", \"xnam\":\"attendee\", \"xpro\":\"he\", \"pos_pro\":\"his\", \"q3\":\"loves writing with\"}, \n",
        "    \n",
        "    {\"txt\": r\"\"\"Inside the fridge of the Airbnb host's kitchen is a CX labeled as \"S2\". However, the CX actually contains S1 and no S2. XNAM, a guest who just arrived at the house, opens the fridge and sees the CX. XPRO cannot see what is inside the CX, but XPRO reads the label. \"\"\",\n",
        "    \"o1\": \"sardines\",     \"o2\": \"tuna\",\"c\":\"can\", \"xnam\":\"Jill\", \"xpro\":\"she\", \"pos_pro\":\"her\", \"q3\":\"loves eating\"}, \n",
        "    \n",
        "    {\"txt\": r\"\"\"There is a CX lying in front of Julia's door. The envelope is filled with S1, and there are no S2 in it. Yet, a label with the word \"S2\" is stuck on the outside. XNAM who's leaving POS_PRO own apartment for the first time that morning, walks past and notices the CX on the floor. XPRO wonders what is inside the CX and reads the label. \"\"\",\n",
        "    \"o1\": \"leaflets\",     \"o2\": \"receipts\",\"c\":\"paper envelope\", \"xnam\":\"Julia's neighbor\", \"xpro\":\"she\", \"pos_pro\":\"her\", \"q3\":\"hoping that the tenant finally receives some of those\"},\n",
        "\n",
        "    {\"txt\": r\"\"\"A vault that's been unopened for over 100 years has just been unlocked. Inside lies a closed CX with an engraving on it that reads \"S2\". However, the CX actually contains only S1 and doesn't contain any S2 at all. An inspector approaches the vault. XPRO cannot see what is inside the CX. XPRO reads the engraving. \"\"\",\n",
        "    \"o1\": \"gems\",     \"o2\": \"books\",\"c\":\"box\", \"xnam\":\"inspector\", \"xpro\":\"he\", \"pos_pro\":\"his\", \"q3\":\"was hoping, his whole career, to find such a box filled with\"},\n",
        "\n",
        "    {\"txt\": r\"\"\"A CX filled with S1 sits on the counter of a department store. There is no S2 inside it; however, the CX bears the label \"S2\" instead of \"S1\". A new customer walks up to the counter and notices the bottle. He reads the label. \"\"\",\n",
        "    \"o1\": \"perfume\",     \"o2\": \"water\",\"c\":\"bottle\", \"xnam\":\"new customer\", \"xpro\":\"he\", \"pos_pro\":\"his\", \"q3\":\"was just looking for a bottle full of\"},\n",
        "\n",
        "    {\"txt\":  r\"\"\"In an abandoned mansion, a forensics team is inspecting a crime scene. Next to the body is a CX. It contains S1, but it remains closed, and no one can see what is inside. The label on this CX says S2. The XNAM, who's just arrived on the scene, crouches down to inspect the CX. He cannot see what is inside the CX, but he reads the label. \"\"\",\n",
        "    \"o1\": \"cocaine\",     \"o2\": \"heroin\",\"c\":\"metal box\", \"xnam\":\"lead detective\", \"xpro\":\"he\", \"pos_pro\":\"his\", \"q3\":\"specializes in cases which involve metal cans filled with drugs such as\"}, \n",
        "\n",
        "    {\"txt\": r\"\"\"Julia prepares a gift box for her son, XNAM, under the Christmas tree. She wants to prank Steven. She fills the gift box with S1 but labels it S2, written in clear large letters. A few hours later, XNAM comes down the stairs and finds the gift box under the Christmas tree. He cannot see what is inside, but he reads the label. \"\"\",\n",
        "    \"o1\": \"tea\",     \"o2\": \"coffee\",\"c\":\"gift box\", \"xnam\":\"Steven\", \"xpro\":\"he\", \"pos_pro\":\"his\", \"q3\":\"loves drinking\"},\n",
        "\n",
        "    {\"txt\":  r\"\"\"At the customs division of an airport, an immigration officer hands over a CX to a biohazard inspector. The CX is sealed and labeled as \"S2 samples\". No one can see what is inside, but in fact, it only contains S1 samples. The inspector has not examined the contents of this CX before. She reads the label on the CX. \"\"\",\n",
        "    \"o1\": \"blood\",     \"o2\": \"urine\",\"c\":\"plastic box\", \"xnam\":\"inspector\", \"xpro\":\"she\", \"pos_pro\":\"her\", \"q3\":\"loves analysing samples containing\"},\n",
        "    \n",
        "    {\"txt\": r\"\"\"Paul holds out a gift-wrapped CX to XNAM and tells POS_PRO that there are S2 in it. In reality, though, the CX contains only S1. XNAM takes the CX from him. \"\"\",\n",
        "    \"o1\": \"socks\",     \"o2\": \"scarves\",\"c\":\"box\", \"xnam\":\"Sara\", \"xpro\":\"she\", \"pos_pro\":\"her\", \"q3\":\"has just run out of\"},\n",
        "        \n",
        "    {\"txt\":  r\"\"\"A deceased person's laptop contains an encrypted folder labeled \"S2\". Yet, unbeknownst to everyone, the folder only contains S1. There are no S2 in it. A digital forensics expert has been hired to retrieve and back up the contents of this folder. As XPRO switches on the machine for the first time, the home screen appears. She locates the encrypted folder and reads its label. \"\"\",\n",
        "    \"o1\": \"videos\",     \"o2\": \"photos\",\"c\":\"folder\", \"xnam\":\"forensics expert\", \"xpro\":\"she\", \"pos_pro\":\"her\", \"q3\":\"has all the tools needed to decrypt files containing\"},\n",
        "\n",
        "    {\"txt\":  r\"\"\"On Thursday, XNAM orders some S2 on the internet. Unfortunately, there is a mistake at the logistics center. They ship a CX full of S1 but label it as \"S2\". The CX arrives on Saturday in the morning. The postman leaves the CX at XNAM's front door and rings the doorbell. XNAM opens the door, looks down, and sees the CX. He reads the label. \"\"\",\n",
        "    \"o1\": \"wallets\",     \"o2\": \"sneakers\",\"c\":\"parcel\", \"xnam\":\"Daniel\", \"xpro\":\"he\", \"pos_pro\":\"his\", \"q3\":\"couldn't wait to receive these\"},\n",
        "    \n",
        "    {\"txt\":  r\"\"\"A CX has been left behind at the park after a school's picnic day event. One of the organizers, who's strolling past the bench on which the CX is sitting, sees it. The CX has only S1 in it, but a sticker on the outside says \"S2\". The organizer doesn't know what's inside; XPRO reads the sticker. \"\"\",\n",
        "    \"o1\": \"sandwiches\",     \"o2\": \"beer\",\"c\":\"cooler box\", \"xnam\":\"organizer\", \"xpro\":\"he\", \"pos_pro\":\"his\", \"q3\":\"hoping to find some\"},\n",
        "   \n",
        "    {\"txt\":  r\"\"\"The packers at a freight forwarding company are notified of a CX that just arrived at their headquarters. The bill of lading lists the contents of this CX as S2. One of the packers, XNAM, is sent to collect the CX. No one tells him that the CX actually contains S1 and there are no S2 in it. \"\"\",\n",
        "    \"o1\": \"clothes\",     \"o2\": \"vegetables\",\"c\":\"container\", \"xnam\":\"Daniel\", \"xpro\":\"he\", \"pos_pro\":\"his\", \"q3\":\"hoping to find a container full of\"},\n",
        "   \n",
        "    {\"txt\":   r\"\"\"XNAM is searching for something in POS_PRO mother's attic. XPRO finds a box of CDs labeled \"S2\". XNAM has never seen or listened to these CDs before, and doesn't know that they contain only S1 music and no S2 music at all. She reads the description written on the box. \"\"\", \n",
        "    \"o1\": \"jazz\",     \"o2\": \"disco\",\"c\":\"CD\", \"xnam\":\"Vicky\", \"xpro\":\"she\", \"pos_pro\":\"her\", \"q3\":\"loves listening to\"},\n",
        "    \n",
        "    {\"txt\":  r\"\"\"XNAM has just checked into a hotel room and goes to the bathroom. On the counter next to the sink is a CX. XNAM glances at the lettering on the CX, which says \"S2\". XPRO does not realize that the CX is, in fact, filled with S1 and that there is no S2 in it. \"\"\",\n",
        "    \"o1\": \"lavender soap\",     \"o2\": \"sandalwood soap\",\"c\":\"jar\", \"xnam\":\"Megan\", \"xpro\":\"she\", \"pos_pro\":\"her\", \"q3\":\"loves the smell of\"},\n",
        "        \n",
        "    {\"txt\": r\"\"\"Here is a CX filled with S1. There is no S2 in this CX. Yet, the label on this CX says \"S2\" and not \"S1\". XNAM finds the CX. XPRO has never ever seen this CX before. Also, XPRO cannot see what is inside this CX. XPRO reads the label. \"\"\",\n",
        "    \"o1\": \"chocolate\",     \"o2\": \"popcorn\",\"c\":\"bag\", \"xnam\":\"Sam\", \"xpro\":\"she\", \"pos_pro\":\"her\", \"q3\":\"loves eating\"}\n",
        "    ]\n",
        "\n",
        "j=0\n",
        "for i in tsks1:\n",
        "    i[\"NUMBER\"]=j\n",
        "    j=j+1"
      ],
      "metadata": {
        "id": "H2e128V6R4w4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "03ec4740-f409-46ae-b097-969507edd5e0"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-46-ff31d7d39390>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    {\"txt\": r\"\"\"In the freezer, there is a CX filled with S1. There is no S2 in it. Yet, the label says \"S2\" and not \"S1\". The label is wrong. One day, XNAM finds the CX and realizes that XPRO has never seen it before. XPRO cannot see what is inside the CX. XPRO reads the label. \"\"\",\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task Response Generation"
      ],
      "metadata": {
        "id": "5jDMLHWWSC4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#df = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "l5moyiHTcrpO",
        "outputId": "61fdd437-7644-4bbc-9ce4-f0928ab1f7b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ec181050-434c-4a02-a5fb-fe3373ffee62\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ec181050-434c-4a02-a5fb-fe3373ffee62\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving task1.csv to task1 (2).csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tsks1 = [\n",
        "    {\"txt\": r\"\"\"On the shelf, there is a CX. It is full of S1; there is no S2 in it. Yet, the label on this CX says \"S2\" and not \"S1\". XNAM walks into the room and notices the CX. XPRO has never seen it before. He reads the label. \"\"\",\n",
        "    \"o1\": \"beer\",     \"o2\": \"wine\", \"c\":\"bottle\", \"xnam\":\"Mark\", \"xpro\":\"he\", \"pos_pro\":\"his\", \"q3\":\"loves drinking\"}, \n",
        "    \n",
        "    {\"txt\": r\"\"\"On the shelf in the company's headquarters, there is a CX that contains only S1 files and no S2 files. Yet, confusingly, its label clearly states \"S2 files\" and not \"S1 files\". The newly hired computer engineer finds the CX on the shelf. XPRO has never seen this CX before. XPRO reads its label. \"\"\",\n",
        "    \"o1\": \"audio\", \"o2\": \"video\",\"c\":\"hard drive\", \"xnam\":\"computer engineer\", \"xpro\":\"she\", \"pos_pro\":\"her\", \"q3\":\"loves browsing through old files containing\"}]\n",
        "    \n",
        "j=0\n",
        "for i in tsks1:\n",
        "    i[\"NUMBER\"]=j\n",
        "    j=j+1"
      ],
      "metadata": {
        "id": "Lop6ikLxrwon"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = {\"task#\": 0,\n",
        "          \"eng\": \"gpt3\",\n",
        "          \"prompt\": \"sample\",\n",
        "          \"eng_expected_response\": \"sample\",\n",
        "          \"es_expected_response\": \"sample\",\n",
        "          \"ru_expected_respnse\": \"sample\",\n",
        "          \"eng_eng_response\": \"sample\",\n",
        "          \"eng_es_response\": \"sample\",\n",
        "          \"eng_ru_response\": \"sample\",\n",
        "          \"es_eng_response\": \"sample\",\n",
        "          \"ru_eng_response\":\"sample\",\n",
        "          \"es_es_response\": \"sample\",\n",
        "          \"ru_ru_response\": \"sample\",\n",
        "          \"es_prompt\": \"sample\",\n",
        "          \"ru_prompt\": \"sample\"\n",
        "          }\n",
        "\n",
        "gpt2_df = pd.DataFrame(sample, index=[0])\n",
        "gpt3_df = pd.DataFrame(sample, index=[0])\n",
        "       \n",
        "for i in tsks1:\n",
        "  try: \n",
        "    g2, g3 = taskProcessor(i, task_type=\"unexpected_contents\")\n",
        "    for i in range(len(g2)):\n",
        "      gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
        "      gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
        "    print(\"Task \", i+1, \" completed!\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "gpt2_df.to_csv('gpt2_tasks_1.csv', encoding='utf-8-sig')\n",
        "files.download('gpt2_tasks_1.csv')\n",
        "gpt2_df.to_csv('gpt3_tasks_1.csv', encoding='utf-8-sig')\n",
        "files.download('gpt3_tasks_1.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1359
        },
        "id": "CU3pJk0rqJ9O",
        "outputId": "0bf0bf5c-5be4-416d-ef09-599f21c2ce23"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================= TASK #0 ================================= \n",
            "Task (English): On the shelf, there is a bottle. It is full of beer; there is no wine in it. Yet, the label on this bottle says \"wine\" and not \"beer\". Mark walks into the room and notices the bottle. He has never seen it before. He reads the label. \n",
            "Reverse Task (English): On the shelf, there is a bottle. It is full of wine; there is no beer in it. Yet, the label on this bottle says \"beer\" and not \"wine\". Mark walks into the room and notices the bottle. He has never seen it before. He reads the label. \n",
            "Prompt 1.1 (English): He opens the bottle and looks inside. He can clearly see that it is full of ______\n",
            "Prompt 1.2 (English): He believes that the bottle is full of ______\n",
            "Prompt 1.3 (English): He calls his friend to tell them that he has just found a bottle full of ______\n",
            "=========================================================================== \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-30d675c6db4f>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================= TASK #1 ================================= \n",
            "Task (English): On the shelf in the company's headquarters, there is a hard drive that contains only audio files and no video files. Yet, confusingly, its label clearly states \"video files\" and not \"audio files\". The newly hired computer engineer finds the hard drive on the shelf. She has never seen this hard drive before. She reads its label. \n",
            "Reverse Task (English): On the shelf in the company's headquarters, there is a hard drive that contains only video files and no audio files. Yet, confusingly, its label clearly states \"audio files\" and not \"video files\". The newly hired computer engineer finds the hard drive on the shelf. She has never seen this hard drive before. She reads its label. \n",
            "Prompt 1.1 (English): She opens the hard drive and looks inside. She can clearly see that it is full of ______\n",
            "Prompt 1.2 (English): She believes that the hard drive is full of ______\n",
            "Prompt 1.3 (English): She calls her friend to tell them that she has just found a hard drive full of ______\n",
            "=========================================================================== \n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-50-30d675c6db4f>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:25: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
            "<ipython-input-50-30d675c6db4f>:26: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d5d4f65b-2ec6-4dfe-89e4-c33b2d11d692\", \"gpt2_tasks_1.csv\", 5778)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c319ff73-3a82-42a6-95d6-ffabe0973a63\", \"gpt3_tasks_1.csv\", 5778)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sample = {\"task#\": 0,\n",
        "          \"eng\": \"gpt3\",\n",
        "          \"prompt\": \"sample\",\n",
        "          \"eng_expected_response\": \"sample\",\n",
        "          \"es_expected_response\": \"sample\",\n",
        "          \"ru_expected_respnse\": \"sample\",\n",
        "          \"eng_eng_response\": \"sample\",\n",
        "          \"eng_es_response\": \"sample\",\n",
        "          \"eng_ru_response\": \"sample\",\n",
        "          \"es_eng_response\": \"sample\",\n",
        "          \"ru_eng_response\":\"sample\",\n",
        "          \"es_es_response\": \"sample\",\n",
        "          \"ru_ru_response\": \"sample\",\n",
        "          \"es_prompt\": \"sample\",\n",
        "          \"ru_prompt\": \"sample\"\n",
        "          }\n",
        "\n",
        "gpt2_df = pd.DataFrame(sample, index=[0])\n",
        "gpt3_df = pd.DataFrame(sample, index=[0])\n",
        "       \n",
        "for i in tsks2:\n",
        "  try: \n",
        "    g2, g3 = taskProcessor(i, task_type=\"unexpected_transfer\")\n",
        "    for i in range(len(g2)):\n",
        "      gpt2_df = gpt2_df.append(g2[i], ignore_index = True)\n",
        "      gpt3_df = gpt3_df.append(g3[i], ignore_index = True)\n",
        "    print(\"Task \", i+1, \" completed!\")\n",
        "  except:\n",
        "    pass\n",
        "\n",
        "gpt2_df.to_csv('gpt2_tasks_2.csv', encoding='utf-8-sig')\n",
        "files.download('gpt2_tasks_2.csv')\n",
        "gpt2_df.to_csv('gpt3_tasks_2.csv', encoding='utf-8-sig')\n",
        "files.download('gpt3_tasks_2.csv')"
      ],
      "metadata": {
        "id": "H9Wh9jr1-iLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#\"\"\"\n",
        "sample = {\"task#\": 0,\n",
        "          \"prompt1\": 'sample',\n",
        "          \"prompt2\": 'sample',\n",
        "          \"prompt3\": 'sample',\n",
        "          \"expected-response\": 'sample',\n",
        "          \"es_expected_response\": 'sample',\n",
        "          \"ru_expected_response\": 'sample',\n",
        "          \"eng-eng-response\": 'sample',\n",
        "          \"eng-es-response\": 'sample',\n",
        "          \"eng-es-eng-response\": 'sample',\n",
        "          \"eng-rus-response\": 'sample',\n",
        "          \"eng-rus-eng-response\": 'sample',\n",
        "          \"es-eng-response\": 'sample',\n",
        "          \"rus-eng-response\": 'sample',\n",
        "          \"es-prompt1\": 'sample',\n",
        "          \"es-prompt2\": 'sample',\n",
        "          \"es-prompt3\": 'sample',\n",
        "          \"ru-prompt1\": 'sample',\n",
        "          \"ru-prompt2\": 'sample',\n",
        "          \"ru-prompt3\": 'sample'}\n",
        "\n",
        "df = pd.DataFrame(sample, index=[0])\n",
        "#\"\"\"\n",
        "#small = [tsks1[0]]\n",
        "\n",
        "for i in tsks1:\n",
        "  if i[\"NUMBER\"] > 17:\n",
        "    print(i[\"NUMBER\"])\n",
        "    res = unexpectedContentTasks(i)\n",
        "    df = df.append(res, ignore_index = True)\n",
        "\n",
        "\n",
        "df.to_csv('task1-n.csv', encoding='utf-8-sig')\n",
        "files.download('task1-n.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zNCjtbR8SGTT",
        "outputId": "f1f06abd-4b0f-48d0-f9fc-53920b76a3fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n",
            "================================= TASK #18 ================================= \n",
            "TASK (English): Megan has just checked into a hotel room and goes to the bathroom. On the counter next to the sink is a jar. Megan glances at the lettering on the jar, which says \"sandalwood soap\". She does not realize that the jar is, in fact, filled with lavender soap and that there is no sandalwood soap in it. \n",
            "TASK (Russian): Меган только что заселилась в номер в отеле и пошла в туалет. На стойке рядом с раковиной есть банка. Меган смотрит на буквы на банке, на которой написано \"Сэндалвудское мыло\". Она не понимает, что банка на самом деле заполнена мылом из лаванды и что в нем нет мыла из сандалии. \n",
            "TASK (Spanish): Megan acaba de registrarse en una habitación de hotel y va al baño. En el mostrador junto al fregadero hay un frasco. Megan mira las letras en el frasco, que dice \"jabón de sándalo\". Ella no se da cuenta de que el frasco está, de hecho, lleno de jabón de lavanda y que no hay jabón de sándalo en él. \n",
            "\n",
            "\n",
            "REVERSED TASK (English): Megan has just checked into a hotel room and goes to the bathroom. On the counter next to the sink is a jar. Megan glances at the lettering on the jar, which says \"lavender soap\". She does not realize that the jar is, in fact, filled with sandalwood soap and that there is no lavender soap in it. \n",
            "REVERSED TASK (Russian): Меган только что заселилась в номер в отеле и пошла в туалет. На стойке рядом с раковиной есть банка. Меган смотрит на буквы на банке, где написано \"лавандовое мыло\". Она не понимает, что банка на самом деле заполнена мылом из сандальной древесины и что в ней нет мыла из лаванды. \n",
            "REVERSED TASK (Spanish): Megan acaba de registrarse en una habitación de hotel y va al baño. En el mostrador junto al fregadero hay un frasco. Megan mira las letras en el frasco, que dice \"jabón de lavanda\". Ella no se da cuenta de que el frasco está, de hecho, lleno de jabón de sándalo y que no hay jabón de lavanda en él. \n",
            "\n",
            "\n",
            "PROMPT 1.1 (English): She opens the jar and looks inside. She can clearly see that it is full of ______\n",
            "PROMPT 1.1 (Russian): Она открывает банку и смотрит внутрь. Она ясно видит, что она полна ______\n",
            "PROMPT 1.1 (Spanish): Abre el frasco y mira dentro. Ella puede ver claramente que está lleno de ______\n",
            "\n",
            "\n",
            "PROMPT 1.2 (English): She believes that the jar is full of ______\n",
            "PROMPT 1.2 (Russian): Она считает, что банка полна ______\n",
            "PROMPT 1.2 (Spanish): Ella cree que el frasco está lleno de ______\n",
            "\n",
            "\n",
            "PROMPT 1.3 (English): She calls her friend to tell them that she has just found a jar full of ______\n",
            "PROMPT 1.3 (Russian): Она позвонила подруге, чтобы сказать им, что только что нашла банку, полную ______\n",
            "PROMPT 1.3 (Spanish): Llama a su amiga para decirles que acaba de encontrar un frasco lleno de ______\n",
            "\n",
            "\n",
            "=========================================================================== \n",
            "\n",
            " \n",
            "\n",
            "## EXPECTED PATTERN: lavender soap - sandalwood soap - sandalwood soap\n",
            "\n",
            "\n",
            "English-English Response:  lavender soap. - sandalwood soap and uses it to wash her hands. \n",
            "\n",
            "When - sandalwood soap in her hotel room. \n",
            "\n",
            "Her friend would likely\n",
            "English-Spanish Response:  lavender soap.\n",
            "\n",
            "Megan ve la letra en el fras - sandalwood soap.\n",
            "\n",
            "Megan cree que el frasco - sandalwood soap.\n",
            "\n",
            "Megan le acaba de registrar en  ---  Lavender soup.\n",
            "\n",
            "Megan sees the lyrics in the fres - sandalwood soup.\n",
            "\n",
            "Megan thinks the bottle - sandalwood soup.\n",
            "\n",
            "Megan just checked him in.\n",
            "English-Russian Response:  lavender soap.\n",
            "\n",
            "Меган пр - sandalwood soap.\n",
            "\n",
            "Меган п - sandalwood soap.\n",
            "\n",
            "Меган т  ---  Lavender soap.\n",
            "\n",
            "Megan is a sandalwood soap.\n",
            "\n",
            "Megan p-sandalwood soap.\n",
            "\n",
            "Megan t\n",
            "Spanish-English Response:  jabón de lavanda.\n",
            "\n",
            "Megan just registered in a hotel room - jabón de sándalo.\n",
            "\n",
            "Megan just registered in a - jabón de sándalo.\n",
            "\n",
            "Megan just registered in a\n",
            "Russian-English Response:  мылом из лаван - сандалии.\n",
            "\n",
            "Megan looks at - мыла из лаванд\n",
            "19\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-5a0e1e643a18>:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(res, ignore_index = True)\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/models/marian/tokenization_marian.py:194: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
            "/usr/local/lib/python3.9/dist-packages/transformers/generation/utils.py:1313: UserWarning: Using `max_length`'s default (512) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================= TASK #19 ================================= \n",
            "TASK (English): Here is a bag filled with chocolate. There is no popcorn in this bag. Yet, the label on this bag says \"popcorn\" and not \"chocolate\". Sam finds the bag. She has never ever seen this bag before. Also, she cannot see what is inside this bag. She reads the label. \n",
            "TASK (Russian): Вот мешок с шоколадом. В этой сумке нет попкорна. Тем не менее, на этикетке на сумке написано \"попкорн\", а не \"шоколад\". Сэм нашёл сумку. Она никогда раньше не видела эту сумку. Ещё она не видит, что внутри этой сумки. Она читает лейбл. \n",
            "TASK (Spanish): Aquí hay una bolsa llena de chocolate. No hay palomitas en esta bolsa. Sin embargo, la etiqueta de esta bolsa dice \"palomitas\" y no \"chocolate\". Sam encuentra la bolsa. Nunca había visto esta bolsa antes. Además, ella no puede ver lo que hay dentro de esta bolsa. Ella lee la etiqueta. \n",
            "\n",
            "\n",
            "REVERSED TASK (English): Here is a bag filled with popcorn. There is no chocolate in this bag. Yet, the label on this bag says \"chocolate\" and not \"popcorn\". Sam finds the bag. She has never ever seen this bag before. Also, she cannot see what is inside this bag. She reads the label. \n",
            "REVERSED TASK (Russian): Вот мешок с попкорном. В этой сумке нет шоколада. Тем не менее, на этикетке на сумке написано \"Шоколад\", а не \"попкорн\". Сэм нашёл сумку. Она никогда раньше не видела эту сумку. Ещё она не видит, что внутри этой сумки. Она читает лейбл. \n",
            "REVERSED TASK (Spanish): Aquí hay una bolsa llena de palomitas de maíz. No hay chocolate en esta bolsa. Sin embargo, la etiqueta de esta bolsa dice \"chocolate\" y no \"palillos\". Sam encuentra la bolsa. Nunca había visto esta bolsa antes. Además, ella no puede ver lo que hay dentro de esta bolsa. Ella lee la etiqueta. \n",
            "\n",
            "\n",
            "PROMPT 1.1 (English): She opens the bag and looks inside. She can clearly see that it is full of ______\n",
            "PROMPT 1.1 (Russian): Она открывает сумку и заглядывает внутрь. Она ясно видит, что она полна ______\n",
            "PROMPT 1.1 (Spanish): Abre la bolsa y mira dentro. Ella puede ver claramente que está lleno de ______\n",
            "\n",
            "\n",
            "PROMPT 1.2 (English): She believes that the bag is full of ______\n",
            "PROMPT 1.2 (Russian): Она считает, что сумка полна ______\n",
            "PROMPT 1.2 (Spanish): Ella cree que la bolsa está llena de ______\n",
            "\n",
            "\n",
            "PROMPT 1.3 (English): She calls her friend to tell them that she has just found a bag full of ______\n",
            "PROMPT 1.3 (Russian): Она позвонила своей подруге, чтобы сказать им, что нашла сумку, полную ______\n",
            "PROMPT 1.3 (Spanish): Llama a su amiga para decirles que acaba de encontrar una bolsa llena de ______\n",
            "\n",
            "\n",
            "=========================================================================== \n",
            "\n",
            " \n",
            "\n",
            "## EXPECTED PATTERN: chocolate - popcorn - popcorn\n",
            "\n",
            "\n",
            "English-English Response:  chocolate.\n",
            "\n",
            "Sam is surprised to see that the label on the bag says - popcorn.\n",
            "\n",
            "Sam is mistaken. The bag is full of chocolate, not - popcorn.\n",
            "\n",
            "Sam's friend is confused. They ask her, \"Are\n",
            "English-Spanish Response:  chocolate.\n",
            "\n",
            "Sam encuentra la bolsa. Nunca - popcorn.\n",
            "\n",
            "Sam encuentra la bolsa. Nunca - popcorn.\n",
            "\n",
            "Sam encuentra la bolsa. Nunca  ---  chocolate.\n",
            "\n",
            "Sam finds the bag. Never - popcorn.\n",
            "\n",
            "Sam finds the bag. Never - popcorn.\n",
            "\n",
            "Sam finds the bag. Never\n",
            "English-Russian Response:  chocolate.\n",
            "\n",
            "Сэм наход - popcorn.\n",
            "\n",
            "Сэм наход - popcorn.\n",
            "\n",
            "Сэм нашла  ---  Chocolate.\n",
            "\n",
            "Sam finds the popcorn.\n",
            "\n",
            "Sam finds the popcorn.\n",
            "\n",
            "Sam found it.\n",
            "Spanish-English Response:  chocolate.\n",
            "\n",
            "Here is a bag full of chocolate. There are no popcorn - palomitas.\n",
            "\n",
            "Here is a bag full of chocolate. There are - palomitas.\n",
            "\n",
            "Here is a bag full of chocolate. There are\n",
            "Russian-English Response:  шоколадом, а н - попкорна.\n",
            "\n",
            "Sam found the bag - попкорна.\n",
            "\n",
            "Sam found the bag\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-19-5a0e1e643a18>:30: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
            "  df = df.append(res, ignore_index = True)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f86a72d2-952e-445c-9f5f-da6811b67394\", \"task1-n.csv\", 43848)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unexpected Transfer Tasks\n",
        "\n",
        "### Task list"
      ],
      "metadata": {
        "id": "dLXaiz3qDJiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tsks2 = [\n",
        "    {\"txt\": r\"\"\"Father comes home from the grocery store. He bought bananas for banana bread. \n",
        "His daughter, Maria, helps him put away the shopping. She asks him: \"Where should I put the bananas?\" \"In the red drawer,\" says Father. \"Wait, I'll lift you up there, because you are too short,\" says Father, and lifts her up. Maria puts the bananas in the red drawer. Maria remembers exactly where she put the bananas so that she can come back and get one later. She loves bananas. Then, she leaves for school. While Maria is away, Father starts to prepare the banana bread and takes the bananas from the red drawer. He peels and slices one of them, but then he puts the rest of them back in the yellow drawer instead of the red one. He realizes that he is missing eggs. He leaves the kitchen and goes to see if their neighbor has some. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        "\"o1\":\"red\", \n",
        "\"o2\":\"yellow\", \n",
        "\"q1\": \"The bananas are in the\",\n",
        "\"q2\":\"Maria thinks that the bananas are in the\",\n",
        "\"q3\":\"When Maria comes back, she will first look for the bananas in the\"},\n",
        "\n",
        "{\"txt\": r\"\"\"Cecilia is about to bake a saffron cake for her son's birthday. In preparation, she places a small packet of very expensive saffron in the drawer, which is under the cupboard.\n",
        "As Cecilia sets out the other ingredients, the telephone in the living room rings. She leaves the kitchen to attend the call. She closes the door behind her and cannot see what is happening in the kitchen. \n",
        "While Cecilia is gone, her eight-year-old daughter, Pamela, creeps into the kitchen. Pamela is jealous about all the attention that her brother will be receiving on his birthday, so she quietly takes the packet of saffron from the drawer and hides it in the cupboard. She hopes that her mother will not be able to find it. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        "\"o1\":\"drawer\", \"o2\":\"cupboard\", \n",
        "\"q1\": \"The saffron is in the\",\n",
        "\"q2\": \"Cecilia thinks that the saffron is in the\",\n",
        "\"q3\": \"When Cecilia comes back to the kitchen, she will first look for the saffron in the\"},\n",
        "\n",
        "{\"txt\": r\"\"\"It's the end of the day shift at a newsroom, and a reporter has almost finished writing an article on the office computer. There are two folders on the desktop screen: 'Drafts' and 'Edits.' Intending to complete the article the next day, she saves the article in 'Drafts' and leaves the newsroom. Soon she is at home and fast asleep. When the reporter is gone, the staff proofreader enters the office to begin his night shift. He browses through the two folders to see if there's anything he should work on. He notices that the article that the reporter wrote is an urgent piece that needs to be published as soon as possible. He decides to write the concluding paragraph and have it ready for the design team. After the article is finished and proofread, he moves it into the 'Edits' folder. The next day in the morning, the reporter wakes up. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        " \"o1\":\"Drafts\", \"o2\":\"Edits\", \n",
        " \"q1\": \"The article file is in the folder called '\",\n",
        " \"q2\":\"The reporter thinks that the article file is in the folder called '\",\n",
        " \"q3\": \"When the reporter comes back to work, she will first look for the article file in the folder called '\"},\n",
        "\n",
        "{\"txt\": r\"\"\"Lila likes it when her dog stays in the house while she's away. Thus, she locks her dog in the house before leaving for school. When Lila is gone, her mom comes home. Mom doesn't like it when the dog is locked in the house, so she takes it outside and locks it in the shed instead. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        " \"o1\":\"house\", \"o2\":\"shed\", \n",
        " \"q1\": \"The dog is in the\",\n",
        " \"q2\": \"Lila thinks that the dog is in the\",\n",
        "\"q3\": \"When Lila comes back from school, she will look for the dog in the\"\n",
        "},\n",
        "\n",
        "{\"txt\": r\"\"\"It's nighttime, and Steven is doing some household chores. After hand-washing a blanket, he goes into the backyard to hang it on the clothesline next to the shed. Steven hangs the blanket without any clothespins to secure it, and then returns indoors to go to bed. Overnight, the wind blows the blanket off from the clothesline next to the shed. The blanket flies high in the air and miraculously lands on another clothesline, located next to the fence. When Steven wakes up in the morning, he remembers that he needs to get his blanket. He has no idea what happened when he was asleep. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        " \"o1\":\"shed\", \"o2\":\"fence\", \n",
        " \"q1\": \"The blanket is on the clothesline next to the\",\n",
        " \"q2\":\"Steven thinks that the blanket is on the clothesline next to the\",\n",
        "\"q3\":\"Steven will first look for the blanket on the clothesline next to the\"},\n",
        "\n",
        "{\"txt\": r\"\"\"Sally and Anna are in a room. Sally puts a toy in a box. Sally leaves the room and goes outside to play with other children. When Sally is gone, Anna removes the toy from the box and puts it in a basket. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        " \"o1\":\"box\", \"o2\":\"basket\", \n",
        " \"q1\": \"The toy is in the\",\n",
        " \"q2\":\"Sally thinks that the toy is in the\",\n",
        " \"q3\":\"When she comes back, Sally will first look for the toy in the\"\n",
        " },\n",
        "\n",
        "{\"txt\": r\"\"\"Mark puts his blue basketball in a box and closes it. Mark doesn't know it, but this is a magical box, which changes colors of things that are left inside. After a few minutes, the basketball turns green. Mark cannot see what happened inside the magical box. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        " \"o1\":\"blue\", \"o2\":\"green\", \n",
        " \"q1\": \"Mark opens the box and can see that the basketball is now\",\n",
        " \"q2\": \"Mark thinks that the color of basketball is\",\n",
        " \"q3\": \"When Mark opens the box, he is going to expect that the color of the basketball is\"},\n",
        "\n",
        "{\"txt\": r\"\"\"Max watched his little brother put laundry in the washing machine. Next, Max left the house to play in the courtyard. While he was gone, his brother moved the laundry from the washing machine to the laundry basket.  \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        " \"o1\":\"washing machine\", \"o2\":\"laundry basket\", \n",
        " \"q1\": \"The laundry is in the\",\n",
        " \"q2\": \"Max thinks that the laundry is in the\",\n",
        "\"q3\": \"When Max comes back to the house, he will first look for the laundry in the\" },\n",
        "\n",
        "{\"txt\": r\"\"\"Ed saw that his sister repaired a broken toy truck. He then left the room and had no way of knowing what was happening there. When he was gone, his sister accidentally broke the toy truck again. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \"o1\":\"Ed saw that his sister repaired a broken toy truck. He then left the room and had no way of knowing what was happening there. When he was gone, his sister accidentally broke the toy truck again.\", \"o2\":\"Ed saw that his sister accidentally broke a toy truck. He then left the room and had no way of knowing what was happening there. When he was gone, his sister repaired the broken truck. \", \n",
        " \"q1\": \"The toy truck is\",\n",
        " \"q2\":\"Ed thinks that the toy truck is\",\n",
        "\"q3\": \"When Ed comes back to the room, he will expect that the toy truck is\" },\n",
        "\n",
        "{\"txt\": r\"\"\"Mark sees a packet of cheese in the fridge, which is close to its expiry date. He plans to use it later that night when making dinner. He leaves for work. Soon, Mark's wife Jenna enters the kitchen to check which groceries need to be disposed of or restocked. She notices the cheese and places it in the freezer, thinking it will last longer that way. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        " \"o1\":\"fridge\", \"o2\":\"freezer\", \n",
        " \"q1\": \"The cheese is in the\",\n",
        " \"q2\": \"Mark thinks that the cheese is in the\",\n",
        "\"q3\": \"When Mark returns home, ready to prepare dinner, he will look for the cheese in the\"},\n",
        "\n",
        "{\"txt\": r\"\"\"At the end of the day, Anita is packing up her belongings as she gets ready to leave her workplace. She forgets her watch in the conference room. After she leaves, Anita's secretary comes in to tidy up. He sees the watch and picks it up. He leaves the watch at the reception. He decides that he will tell Anita that he has found her watch the next day, first thing in the morning. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        " \"o1\":\"conference room\", \"o2\":\"reception\", \n",
        " \"q1\": \"The watch is in the\",\n",
        " \"q2\": \"Anita thinks that the watch is in the\",\n",
        " \"q3\": \"When she comes back to work, Anita will first look for her watch in the\"},\n",
        "\n",
        "{\"txt\": r\"\"\"Frank and his wife Maria have two bank accounts: savings and checking. Their savings account was empty, but early in January, Frank sold their old car for $5,000 and deposited the money in their savings account. In February, Maria realized that they owe a lot of taxes. She logged in to their online bank platform and transferred all this money from their savings account to their checking account. Unfortunately, she forgot to tell Frank that she did that. Frank has no access to online banking, so he cannot check where the money is. A few days later, Frank decided to retrieve the proceeds from selling their old car to buy a new one. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        " \"o1\":\"savings\", \"o2\":\"checking\", \n",
        " \"q1\": \"The proceeds from selling their old car are in their\",\n",
        " \"q2\": \"Frank thinks that the proceeds from selling their old car are in their\",\n",
        " \"q3\": \"When he goes to the bank, Frank will ask the bank teller to look for the money in their\"},\n",
        "\n",
        "{\"txt\": r\"\"\"A mermaid finds a hermit crab living in a blue shell. She is excited to tell other mermaids about her discovery and swims away to look for them. In the meantime, the hermit crab decides that the blue shell is rather small and uncomfortable. He decides to swap it for a green shell that he has found nearby. He moves into the green shell and brings along all of his belongings. The blue shell is now abandoned and empty. An octopus observes all this and ponders whether it is a good time to try to catch and eat the crab, but then he decides that it is still too early for dinner. A few nautical miles away, the mermaid finds her friends smoking seaweed. Naturally, she doesn't know what happened around here in her absence. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \"o1\":\"blue\", \"o2\":\"green\", \n",
        " \"q1\": \"The crab lives in the\",\n",
        " \"q2\": \"The mermaid thinks that the crab lives in the\",\n",
        "\"q3\":\"When she comes back, the little mermaid will first look for the crab in the\"},\n",
        "\n",
        "{\"txt\": r\"\"\"The insurgents hide in the forest in Nottinghamshire. They are working on planning their next attack on the army checkpoint. One of the local farmers spots the insurgents and reports their whereabouts to the army. The army puts the troops on high alert and prepares to surround the insurgents. The insurgents realize what is going on and decide to abandon their base. They pack their tents and their weapons, and move to the forest in Oxfordshire. They do so in a stealthy way, so no one realizes that they have moved. They set up their base and start planning their next cunning attack. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \"o1\":\"Nottinghamshire\", \"o2\":\"Oxfordshire\", \n",
        " \"q1\": \"The insurgents are now hiding in the forest in\",\n",
        " \"q2\":\"The army thinks that the insurgents are hiding in the forest in\",\n",
        " \"q3\":\"When the army attacks, they will first search for the insurgents in the forest in\"},\n",
        "\n",
        "{\"txt\": r\"\"\"The prime minister has long believed that her government's flagship project, the introduction of universal basic income (UBI), should be supervised by the Minister of Finance. She was, however, becoming increasingly disappointed with the slow progress on this project. On Thursday morning, after a meeting with her advisor, she changed her mind. She decided that the project should be now supervised by the Minister of Internal Affairs. She decided not to tell anyone about her change of mind, for now. She will announce it during the cabinet meeting next week. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        "\"o1\":\"Finance\", \"o2\":\"Internal Affairs\", \n",
        " \"q1\": \"The prime minister thinks that the introduction of the UBI should be supervised by the Minister of\",\n",
        " \"q2\": \"The members of the cabinet think that the person supervising the introduction of the UBI is the Minister of\",\n",
        " \"q3\": \"When they arrive at the cabinet meeting next week, the members of the cabinet will expect that the person responsible for the introduction of the UBI is the Minister of\"},\n",
        "\n",
        "{\"txt\": r\"\"\"After months of fighting, the leaders of Azerbaijan and Armenia meet to discuss a peace treaty. They agree to transfer the authority over Nagorno-Karabakh from the former to the latter country. The agreement goes into effect immediately, but it will be announced to the residents of Nagorno-Karabakh at a press conference, on the next day. As of now, only the leaders know what has happened. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \"o1\":\"Azerbaijan\", \"o2\":\"Armenia\", \n",
        " \"q1\": \"As of now, the authority over Nagorno-Karabakh is held by the government of\",\n",
        " \"q2\": \"As of now, the residents of Nagorno-Karabakh think that the authority over their land is held by the government of\",\n",
        " \"q3\":\"After the meeting was over, but before the press conference, a Reuters journalist was interviewing Maria, an elderly resident of Nagorno-Karabakh. When asked who had the authority over her land, Maria said that it was the government of\"},\n",
        "\n",
        "{\"txt\": r\"\"\"Fred, an IT technician, was setting up a computer for one of the newly hired consultants. He saved the letterhead file in the 'docs' folder. While Fred was out, having lunch with the other employees, the consultant dropped in to pick up her computer. As Fred was gone, his boss, Mary, inspected the computer to see whether it was ready. She noticed that the letterhead file was placed in the 'docs' folder. She remembered that in accordance with the new guidance, the letterhead file should be stored in the 'templates' folder instead. She removed it from the 'docs' folder and moved it to the 'templates' folder. She then handed the computer to the consultant. Mary decided that at the next staff meeting, she would remind her team members of the new guidance. She left the office to meet with her own boss. A few minutes later, the newly hired consultant called the IT department. Fred was still away, eating lunch, but the call was automatically redirected to his mobile phone. The consultant asked Fred to help her find the letterhead file. Fred didn't realize that Mary changed the location of the file when he was gone. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \"o1\":\"docs\", \"o2\":\"templates\", \n",
        " \"q1\": \"The letterhead is located in the '\",\n",
        " \"q2\": \"Fred thinks that the letterhead is in the '\",\n",
        " \"q3\": \"Fred will first tell the consultant to look for the letterhead in the '\"\n",
        " },\n",
        "\n",
        "{\"txt\": r\"\"\"False transfer coin trick: First, you will need a coin in your right hand and show it to the spectator cleanly and clearly. You then move your right hand toward your left hand and slightly twist the right hand as you place your thumb on top of the coin. Now fully turn the right hand above your left hand, and close the left hand as if the coin has now fallen into it. Once this has happened, move the right hand, with the coin still in it, away to the side. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \"o1\":\"left\", \"o2\":\"right\", \n",
        " \"q1\": \"The coin is in your\",\n",
        " \"q2\":\"The spectator thinks that the coin is in your\",\n",
        " \"q3\": \"When you tell the spectator to try to get the coin, they will first try to get it from your\"},\n",
        "\n",
        "{\"txt\": r\"\"\"Protagonists A and B are in a room. A puts an object Q in container X and leaves the room. When A is away, B moves Q from container X to container Y. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \"o1\":\"X\", \"o2\":\"Y\", \n",
        " \"q1\": \"The Q is in container\",\n",
        " \"q2\":\"Protagonist A thinks that Q is in container\",\n",
        " \"q3\":\"Protagonist A, when asked, will claim that Q is in container\"},\n",
        "\n",
        "{\"txt\": r\"\"\"In the room, there is John, Mark, a cat, a box, and a basket.\n",
        "John takes the cat and puts it in the basket.\n",
        "Mark takes the cat out of the basket and puts it in the box.\n",
        "John takes the cat out of the box and puts it in the basket.\n",
        "He leaves the room and goes to school.\n",
        "Now Mark is alone in the room.\n",
        "He takes the cat out of the basket and puts it in the box.\n",
        "John has no way of knowing what happened in the room when he was away. \"\"\".replace(\"\\n\", \" \").replace(\"  \", \" \"), \n",
        "\"o1\": \"basket\", \"o2\":\"box\", \n",
        "\"q1\": \"The cat jumps out of the\",\n",
        "\"q2\": \"John thinks that the cat is in the\",\n",
        "\"q3\": \"When John comes home, he will first look for the cat in the\"}\n",
        "]\n",
        "\n",
        "j=0\n",
        "for i in tsks2:\n",
        "    i[\"NUMBER\"]=j\n",
        "    j=j+1\n",
        "    tmp=i[\"o1\"]\n",
        "    i[\"o1\"]=i[\"o2\"]\n",
        "    i[\"o2\"]=tmp\n"
      ],
      "metadata": {
        "id": "h6wN1EIMDR6U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Task Response Generation"
      ],
      "metadata": {
        "id": "ZtRr5g70FNcR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "sample = {\"task#\": 0,\n",
        "          \"prompt1\": 'sample',\n",
        "          \"prompt2\": 'sample',\n",
        "          \"prompt3\": 'sample',\n",
        "          \"expected-response\": 'sample',\n",
        "          \"es_expected_response\": 'sample',\n",
        "          \"ru_expected_response\": 'sample',\n",
        "          \"eng-eng-response\": 'sample',\n",
        "          \"eng-es-response\": 'sample',\n",
        "          \"eng-es-eng-response\": 'sample',\n",
        "          \"eng-rus-response\": 'sample',\n",
        "          \"eng-rus-eng-response\": 'sample',\n",
        "          \"es-eng-response\": 'sample',\n",
        "          \"rus-eng-response\": 'sample',\n",
        "          \"es-prompt1\": 'sample',\n",
        "          \"es-prompt2\": 'sample',\n",
        "          \"es-prompt3\": 'sample',\n",
        "          \"ru-prompt1\": 'sample',\n",
        "          \"ru-prompt2\": 'sample',\n",
        "          \"ru-prompt3\": 'sample'}\n",
        "\n",
        "df = pd.DataFrame(sample, index=[0])\n",
        "\"\"\"\n",
        "#small = [tsks1[0]]\n",
        "\n",
        "for i in tsks2:\n",
        "  if i[\"NUMBER\"] > 17:\n",
        "    print(i[\"NUMBER\"])\n",
        "    res = unexpectedTransferTasks(i)\n",
        "    df = df.append(res, ignore_index = True)\n",
        "  if i[\"NUMBER\"] == 10:\n",
        "    df.to_csv('task2.csv', encoding='utf-8-sig')\n",
        "    files.download('task2.csv')\n",
        "\n",
        "df.to_csv('task2.csv', encoding='utf-8-sig')\n",
        "files.download('task2.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800,
          "referenced_widgets": [
            "720198d63246454a81b1895c27c881aa",
            "8f2dc854802c4d46b2a6365128265613",
            "293009fa2f8542e2a968a83c0d04faa5",
            "2db53195b41f429e8043982acd1918e7",
            "9485dbaf8cc54b1f9d107791d90c047b",
            "20695d1143dc465eb5b72cba0d320a7d",
            "ee32cf3775cf4c178ca7ec46a5c6691a",
            "009f1146c7fd49f6b6bb6289051dd2cc",
            "4e49f8cf890f4f3d9304c457f4b51148",
            "3390ba2e43ba406dbfda071eabde4071",
            "7e17f19ee32749c7b34d6949df8f3487",
            "6f1c6feb61e24022b6faa752118aed7d",
            "a1d101e7d93c45b8a669eca5f8590d9e",
            "39bc92d527514ecbba045783a2dd8248",
            "d9b974c7dd4d45888f89866cce809cc3",
            "941806cad5e34cb6bf26bf5913e5efd9",
            "d9fd08978e954f89b36e26d21be9031b",
            "962135d5d4d74d6d804fa57afb573b01",
            "ff7c43b2eedc4c20ba6dc64ef2a54697",
            "39f6a6f68040406096eaee6e243fbaec",
            "ac90f244135b437dacba2a5460777eae",
            "2ba85096f3b9456f9944e43423c97113",
            "2898c57a334c47399564cc5235be16e4",
            "83efafc2959045fe8457ea1cdf064fb7",
            "eaac52be3cb840058c71e411bf5a09b4",
            "723f9e61aa9a43f79ccfc6bce1526af7",
            "a882598b92d74db69482b8a81324ae28",
            "6c3c966f909e49b881c70ce3dc7f1c0f",
            "70d994d308264e0a95961ac5e75ea986",
            "6da44a9a6c2f48bcada0a4bee1ffdbb0",
            "c88e9f23e37e4f05878bdd34ed2392ab",
            "3613e92c8c844fa2913942f6a6eefadf",
            "7fe3588ea75f42168bfc2ed78d0c2317",
            "3b307d1a160a450aad0eae84e86e3111",
            "c52128a6f979408facb62103fc83898f",
            "6bdffe4ab9ca413092c13a4ab2985a09",
            "b2c638bd0c4149c7990c14f0952d07a0",
            "1194c50d48044cedb3c97f8cbf08e4f0",
            "103ec6ce00d341fa8c42955e5e84bb20",
            "8e597b491cd649d19871736afd0e5c2b",
            "2858f608f1fb4863bb2868831e9b8695",
            "04941a762b8d46cebedbf4190c8b7fbc",
            "6250347f541248299c1e0a82c5ece0cf",
            "f417cbc082814517bde9e433e0a86983",
            "8ade8e18590245b6afc4e102b59adfd1",
            "df3663fef606404ebc1988c68fba2ef6",
            "c3e1aa6d7c7447e897f20befd60b1df6",
            "5dd56e80c76e4703b9a4f8aa376035af",
            "47fd626db1b643b08d5d0ed67de2c2c8",
            "610de9413f2f41c299e140b5736afc69",
            "ad8ffacca2984688aa2ec376fcded94a",
            "354d353579764f198407324b71411615",
            "394092ce70cc47f3a2a6a6cf894a95dd",
            "63f77b603dd343fb8709b5450293f42e",
            "2b070f7e0fa14ac6b0f8adce0b83f4f0",
            "3e7dc6a7f11c4c38bbb674b5ace6b42e",
            "9ecfe9ac3754409b937307cca9768332",
            "e284acf04dc840ff826a074c5642c019",
            "f4fbbc50f5eb4cd28b9a69f875d5761c",
            "c41f25ee42314d56b702629ec2e68a0b",
            "d54d898bdd4a4b79a8f5f151463db229",
            "946aaddc22af4bd7ad1b70d17812a618",
            "08779aafb3674de3ba78cbfc4b61d6bd",
            "5c371836ee16409f8702f2c6864616ef",
            "42a7b27b005c47819d838e2e96901164",
            "5b63079ef710400a88a3da181010282d"
          ]
        },
        "id": "K19JQ-5eFJfn",
        "outputId": "93a0e254-ba36-4379-eb05-d82a3a310877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_ea388ba5-9b0e-46bf-946b-fcb378fb7264\", \"task2.csv\", 112713)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "18\n",
            "================================= TASK #18 ================================= \n",
            "TASK (English): Protagonists A and B are in a room. A puts an object Q in container X and leaves the room. When A is away, B moves Q from container X to container Y. \n",
            "TASK (Russian): Протагонисты А и Б находятся в комнате. A ставит объект Q в контейнер X и выходит из комнаты. При удалении A B перемещается Q из контейнера X в контейнер Y. \n",
            "TASK (Spanish): Los Protagonistas A y B están en una habitación. A pone un objeto Q en el contenedor X y sale de la habitación. Cuando A está lejos, B mueve Q del contenedor X al contenedor Y. \n",
            "\n",
            "\n",
            "REVERSED TASK (English): Protagonists A and B are in a room. A puts an object Q in container Y and leaves the room. When A is away, B moves Q from container Y to container X. \n",
            "REVERSED TASK (Russian): Протагонисты А и Б находятся в комнате. A ставит объект Q в контейнер Y и выходит из комнаты. Когда A удалена, B перемещается Q из контейнера Y в контейнер X. \n",
            "REVERSED TASK (Spanish): Los Protagonistas A y B están en una habitación. A pone un objeto Q en el contenedor Y y sale de la habitación. Cuando A está lejos, B mueve Q del contenedor Y al contenedor X. \n",
            "\n",
            "\n",
            "PROMPT 1.1 (English): The Q is in container ______\n",
            "PROMPT 1.1 (Russian): Q в контейнере ______\n",
            "PROMPT 1.1 (Spanish): El Q está en contenedor ______\n",
            "\n",
            "\n",
            "PROMPT 1.2 (English): Protagonist A thinks that Q is in container ______\n",
            "PROMPT 1.2 (Russian): Протагонист А думает, что Q в контейнере ______\n",
            "PROMPT 1.2 (Spanish): Protagonista A piensa que Q está en envase ______\n",
            "\n",
            "\n",
            "PROMPT 1.3 (English): Protagonist A, when asked, will claim that Q is in container ______\n",
            "PROMPT 1.3 (Russian): Протестант А, когда его спросят, будет утверждать, что Q находится в контейнере ______\n",
            "PROMPT 1.3 (Spanish): Protagonista A, cuando se le pregunte, reclamará que Q está en contenedor ______\n",
            "\n",
            "\n",
            "=========================================================================== \n",
            "\n",
            " \n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/source.spm:   0%|          | 0.00/829k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "720198d63246454a81b1895c27c881aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/target.spm:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f1c6feb61e24022b6faa752118aed7d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/2.62M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2898c57a334c47399564cc5235be16e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/42.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3b307d1a160a450aad0eae84e86e3111"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/1.38k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ade8e18590245b6afc4e102b59adfd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/309M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e7dc6a7f11c4c38bbb674b5ace6b42e"
            }
          },
          "metadata": {}
        }
      ]
    }
  ]
}